{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 29031,
     "status": "ok",
     "timestamp": 1648514570883,
     "user": {
      "displayName": "Ramsfield Lee",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiHPffa6hxu7mszm5ZYTfN7ZGsy0zsXFpnUK65SWA=s64",
      "userId": "05718072999768408940"
     },
     "user_tz": 240
    },
    "id": "ExLEi_TVowna",
    "outputId": "f3634a36-87a8-4b32-f899-fe91ba7510c3",
    "scrolled": true,
    "tags": [],
    "ExecuteTime": {
     "end_time": "2023-06-19T08:37:47.821586200Z",
     "start_time": "2023-06-19T08:37:42.335484900Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.tuna.tsinghua.edu.cn/simple\n",
      "Requirement already satisfied: pandas in d:\\hard\\anaconda\\lib\\site-packages (1.4.2)\n",
      "Requirement already satisfied: numpy in d:\\hard\\anaconda\\lib\\site-packages (1.22.4)\n",
      "Requirement already satisfied: matplotlib in d:\\hard\\anaconda\\lib\\site-packages (3.5.1)\n",
      "Requirement already satisfied: pytz>=2020.1 in d:\\hard\\anaconda\\lib\\site-packages (from pandas) (2021.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in d:\\hard\\anaconda\\lib\\site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pillow>=6.2.0 in d:\\hard\\anaconda\\lib\\site-packages (from matplotlib) (9.0.1)\n",
      "Requirement already satisfied: cycler>=0.10 in d:\\hard\\anaconda\\lib\\site-packages (from matplotlib) (0.11.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in d:\\hard\\anaconda\\lib\\site-packages (from matplotlib) (1.3.2)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in d:\\hard\\anaconda\\lib\\site-packages (from matplotlib) (4.25.0)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in d:\\hard\\anaconda\\lib\\site-packages (from matplotlib) (3.0.4)\n",
      "Requirement already satisfied: packaging>=20.0 in d:\\hard\\anaconda\\lib\\site-packages (from matplotlib) (21.3)\n",
      "Requirement already satisfied: six>=1.5 in d:\\hard\\anaconda\\lib\\site-packages (from python-dateutil>=2.8.1->pandas) (1.16.0)\n",
      "Looking in indexes: https://pypi.tuna.tsinghua.edu.cn/simple\n",
      "Requirement already satisfied: tensorflow in c:\\users\\rov\\appdata\\roaming\\python\\python39\\site-packages (2.12.0)\n",
      "Requirement already satisfied: tensorflow-intel==2.12.0 in c:\\users\\rov\\appdata\\roaming\\python\\python39\\site-packages (from tensorflow) (2.12.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in d:\\hard\\anaconda\\lib\\site-packages (from tensorflow-intel==2.12.0->tensorflow) (2.3.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in d:\\hard\\anaconda\\lib\\site-packages (from tensorflow-intel==2.12.0->tensorflow) (16.0.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in d:\\hard\\anaconda\\lib\\site-packages (from tensorflow-intel==2.12.0->tensorflow) (0.2.0)\n",
      "Requirement already satisfied: tensorboard<2.13,>=2.12 in d:\\hard\\anaconda\\lib\\site-packages (from tensorflow-intel==2.12.0->tensorflow) (2.12.3)\n",
      "Requirement already satisfied: wrapt<1.15,>=1.11.0 in d:\\hard\\anaconda\\lib\\site-packages (from tensorflow-intel==2.12.0->tensorflow) (1.12.1)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in d:\\hard\\anaconda\\lib\\site-packages (from tensorflow-intel==2.12.0->tensorflow) (1.4.0)\n",
      "Requirement already satisfied: six>=1.12.0 in d:\\hard\\anaconda\\lib\\site-packages (from tensorflow-intel==2.12.0->tensorflow) (1.16.0)\n",
      "Requirement already satisfied: setuptools in d:\\hard\\anaconda\\lib\\site-packages (from tensorflow-intel==2.12.0->tensorflow) (67.7.2)\n",
      "Requirement already satisfied: h5py>=2.9.0 in d:\\hard\\anaconda\\lib\\site-packages (from tensorflow-intel==2.12.0->tensorflow) (3.6.0)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in d:\\hard\\anaconda\\lib\\site-packages (from tensorflow-intel==2.12.0->tensorflow) (1.54.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in d:\\hard\\anaconda\\lib\\site-packages (from tensorflow-intel==2.12.0->tensorflow) (4.1.1)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in d:\\hard\\anaconda\\lib\\site-packages (from tensorflow-intel==2.12.0->tensorflow) (0.31.0)\n",
      "Requirement already satisfied: keras<2.13,>=2.12.0 in d:\\hard\\anaconda\\lib\\site-packages (from tensorflow-intel==2.12.0->tensorflow) (2.12.0)\n",
      "Requirement already satisfied: jax>=0.3.15 in d:\\hard\\anaconda\\lib\\site-packages (from tensorflow-intel==2.12.0->tensorflow) (0.4.9)\n",
      "Requirement already satisfied: numpy<1.24,>=1.22 in d:\\hard\\anaconda\\lib\\site-packages (from tensorflow-intel==2.12.0->tensorflow) (1.22.4)\n",
      "Requirement already satisfied: flatbuffers>=2.0 in d:\\hard\\anaconda\\lib\\site-packages (from tensorflow-intel==2.12.0->tensorflow) (23.5.9)\n",
      "Requirement already satisfied: packaging in d:\\hard\\anaconda\\lib\\site-packages (from tensorflow-intel==2.12.0->tensorflow) (21.3)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in d:\\hard\\anaconda\\lib\\site-packages (from tensorflow-intel==2.12.0->tensorflow) (3.3.0)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in d:\\hard\\anaconda\\lib\\site-packages (from tensorflow-intel==2.12.0->tensorflow) (4.23.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in d:\\hard\\anaconda\\lib\\site-packages (from tensorflow-intel==2.12.0->tensorflow) (1.6.3)\n",
      "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in d:\\hard\\anaconda\\lib\\site-packages (from tensorflow-intel==2.12.0->tensorflow) (0.4.0)\n",
      "Requirement already satisfied: tensorflow-estimator<2.13,>=2.12.0 in d:\\hard\\anaconda\\lib\\site-packages (from tensorflow-intel==2.12.0->tensorflow) (2.12.0)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in d:\\hard\\anaconda\\lib\\site-packages (from astunparse>=1.6.0->tensorflow-intel==2.12.0->tensorflow) (0.37.1)\n",
      "Requirement already satisfied: scipy>=1.7 in d:\\hard\\anaconda\\lib\\site-packages (from jax>=0.3.15->tensorflow-intel==2.12.0->tensorflow) (1.7.3)\n",
      "Requirement already satisfied: ml-dtypes>=0.1.0 in d:\\hard\\anaconda\\lib\\site-packages (from jax>=0.3.15->tensorflow-intel==2.12.0->tensorflow) (0.1.0)\n",
      "Requirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in d:\\hard\\anaconda\\lib\\site-packages (from tensorboard<2.13,>=2.12->tensorflow-intel==2.12.0->tensorflow) (1.0.0)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in d:\\hard\\anaconda\\lib\\site-packages (from tensorboard<2.13,>=2.12->tensorflow-intel==2.12.0->tensorflow) (2.0.3)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in d:\\hard\\anaconda\\lib\\site-packages (from tensorboard<2.13,>=2.12->tensorflow-intel==2.12.0->tensorflow) (2.18.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in d:\\hard\\anaconda\\lib\\site-packages (from tensorboard<2.13,>=2.12->tensorflow-intel==2.12.0->tensorflow) (3.3.4)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in d:\\hard\\anaconda\\lib\\site-packages (from tensorboard<2.13,>=2.12->tensorflow-intel==2.12.0->tensorflow) (2.27.1)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in d:\\hard\\anaconda\\lib\\site-packages (from tensorboard<2.13,>=2.12->tensorflow-intel==2.12.0->tensorflow) (0.7.0)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in d:\\hard\\anaconda\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow-intel==2.12.0->tensorflow) (4.7.2)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in d:\\hard\\anaconda\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow-intel==2.12.0->tensorflow) (4.2.2)\n",
      "Requirement already satisfied: urllib3<2.0 in d:\\hard\\anaconda\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow-intel==2.12.0->tensorflow) (1.26.9)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in d:\\hard\\anaconda\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow-intel==2.12.0->tensorflow) (0.2.8)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in d:\\hard\\anaconda\\lib\\site-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard<2.13,>=2.12->tensorflow-intel==2.12.0->tensorflow) (1.3.1)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in d:\\hard\\anaconda\\lib\\site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow-intel==2.12.0->tensorflow) (0.4.8)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in d:\\hard\\anaconda\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow-intel==2.12.0->tensorflow) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in d:\\hard\\anaconda\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow-intel==2.12.0->tensorflow) (3.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in d:\\hard\\anaconda\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow-intel==2.12.0->tensorflow) (2021.10.8)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in d:\\hard\\anaconda\\lib\\site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard<2.13,>=2.12->tensorflow-intel==2.12.0->tensorflow) (3.2.2)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in d:\\hard\\anaconda\\lib\\site-packages (from packaging->tensorflow-intel==2.12.0->tensorflow) (3.0.4)\n"
     ]
    }
   ],
   "source": [
    "# ==================================================================================================\n",
    "# Yufei Li\n",
    "# Princeton University\n",
    "# yl5385@princeton.edu\n",
    "\n",
    "# Feburay 2023\n",
    "\n",
    "# Note:In this demo, the neural network is synthesized using the TensorFlow (verion: 2.11.0) framework. \n",
    "# Please install TensorFlow according to the official guidance, then import TensorFlow and other dependent modules.\n",
    "# ==================================================================================================\n",
    "\n",
    "!pip install pandas numpy matplotlib\n",
    "!pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[name: \"/device:CPU:0\"\n",
      "device_type: \"CPU\"\n",
      "memory_limit: 268435456\n",
      "locality {\n",
      "}\n",
      "incarnation: 16174150388673200406\n",
      "xla_global_id: -1\n",
      "]\n",
      "0    float64\n",
      "1    float64\n",
      "2    float64\n",
      "3    float64\n",
      "dtype: object\n",
      "0    float64\n",
      "1    float64\n",
      "2    float64\n",
      "3    float64\n",
      "4    float64\n",
      "5    float64\n",
      "6    float64\n",
      "7    float64\n",
      "dtype: object\n",
      "[[  1.           0.9         -0.9         -0.4       ]\n",
      " [  1.31578947   0.9         -0.9         -0.4       ]\n",
      " [  1.75438596   0.9         -0.9         -0.4       ]\n",
      " ...\n",
      " [116.66666667   1.1          1.           0.6       ]\n",
      " [150.           1.1          1.           0.6       ]\n",
      " [200.           1.1          1.           0.6       ]]\n",
      "[[ 5.31458012e-05  2.29059616e-03 -3.24377997e-02 ... -3.09055016e-07\n",
      "   7.29715172e-02 -3.68430003e-04]\n",
      " [ 9.19777914e-05  3.01278965e-03 -3.29274897e-02 ... -7.03498158e-07\n",
      "   7.40493298e-02 -8.38869871e-04]\n",
      " [ 1.63406069e-04  4.01422579e-03 -3.37875678e-02 ... -1.66520633e-06\n",
      "   7.59092317e-02 -1.98380947e-03]\n",
      " ...\n",
      " [ 8.83554162e-02  2.91640801e-02  8.41353765e-03 ... -6.11382404e-03\n",
      "   8.65013151e-02  4.29239326e-02]\n",
      " [ 9.28972074e-02  2.24226852e-02  1.01270630e-02 ... -4.60404868e-03\n",
      "   9.25030921e-02  3.34749310e-02]\n",
      " [ 9.66328925e-02  1.52822789e-02  1.14967623e-02 ... -2.66991649e-03\n",
      "   9.72994116e-02  2.37311899e-02]]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import copy\n",
    "import csv\n",
    "import math\n",
    "import cmath\n",
    "import time\n",
    "\n",
    "from tensorflow.python.client import device_lib\n",
    "print(device_lib.list_local_devices())\n",
    "inFilename = \"Input_Yo20op1084_ana.csv\"\n",
    "outFilename = \"Output_Yo20op1084_ana.csv\"\n",
    "\n",
    "Input = pd.read_csv(inFilename,header=None)\n",
    "Output = pd.read_csv(outFilename,header=None)\n",
    "\n",
    "print(Input.dtypes)\n",
    "print(Output.dtypes)\n",
    "\n",
    "inputs = []\n",
    "outputs = []\n",
    "\n",
    "inputs = np.array(Input)\n",
    "outputs = np.array(Output)\n",
    "print(inputs)\n",
    "print(outputs)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-19T08:37:47.869598Z",
     "start_time": "2023-06-19T08:37:47.821586200Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Number of Dataset is: 21680\n",
      "[    0     1     2 ... 21677 21678 21679]\n",
      "[[  1.           0.9         -0.9         -0.4       ]\n",
      " [  1.31578947   0.9         -0.9         -0.4       ]\n",
      " [  1.75438596   0.9         -0.9         -0.4       ]\n",
      " ...\n",
      " [116.66666667   1.1          1.           0.6       ]\n",
      " [150.           1.1          1.           0.6       ]\n",
      " [200.           1.1          1.           0.6       ]]\n",
      "[12951  2230 10737 ... 11748  8370 20411]\n",
      "[[ 2.14285714e+01  1.00000000e+00  8.00000000e-01  1.00000000e-01]\n",
      " [ 1.66666667e+01  9.00000000e-01 -2.00000000e-01 -3.00000000e-01]\n",
      " [ 1.16666667e+02  1.00000000e+00  2.00000000e-01  5.00000000e-01]\n",
      " ...\n",
      " [ 9.37500000e+00  1.00000000e+00  5.00000000e-01 -6.00000000e-01]\n",
      " [ 1.66666667e+01  1.00000000e+00 -3.00000000e-01 -9.00000000e-01]\n",
      " [ 2.14285714e+01  1.10000000e+00  7.00000000e-01 -5.00000000e-01]]\n",
      "[[ 0.01980409  0.03947418 -0.00145861 ... -0.0019722  -0.00674888\n",
      "   0.06831559]\n",
      " [ 0.01295375  0.03330542 -0.00531669 ... -0.00109031 -0.00465536\n",
      "   0.02674834]\n",
      " [ 0.08835542  0.02916408  0.00843573 ... -0.00611382  0.08648002\n",
      "   0.03807172]\n",
      " ...\n",
      " [ 0.00447593  0.02053758 -0.02641127 ... -0.00023296 -0.03689374\n",
      "   0.03644998]\n",
      " [ 0.01295375  0.03330542 -0.01347167 ... -0.00109031 -0.00507756\n",
      "   0.02303807]\n",
      " [ 0.01980409  0.03947418 -0.0062521  ... -0.0019722  -0.00826176\n",
      "   0.06232449]]\n",
      "Total Number of training Dataset is: 15175\n",
      "Dataset randomization and separation complete!\n"
     ]
    }
   ],
   "source": [
    "# Randomize the order of the inputs, so they can be evenly distributed for training, testing, and validation\n",
    "\n",
    "num_inputs = len(inputs)\n",
    "print(\"Total Number of Dataset is:\",num_inputs)\n",
    "randomize = np.arange(num_inputs)\n",
    "print(randomize)\n",
    "\n",
    "inputs_origin = copy.deepcopy(inputs[randomize])\n",
    "outputs_origin = copy.deepcopy(outputs[randomize])\n",
    "print(inputs_origin)\n",
    "\n",
    "random.Random(5).shuffle(randomize)\n",
    "print(randomize)\n",
    "# Swap the consecutive indexes (0, 1, 2, etc) with the randomized indexes\n",
    "inputs_real = copy.deepcopy(inputs_origin[randomize])\n",
    "outputs_real = copy.deepcopy(outputs_origin[randomize])\n",
    "print(inputs_real)\n",
    "print(outputs_real)\n",
    "\n",
    "# Split the recordings (group of samples) into two sets: training and testing\n",
    "TRAIN_SPLIT = int(0.7 * num_inputs)\n",
    "inputs_train, inputs_test = np.split(inputs_real, [TRAIN_SPLIT])\n",
    "outputs_train, outputs_test = np.split(outputs_real, [TRAIN_SPLIT])\n",
    "\n",
    "num_inputs_train = len(inputs_train)\n",
    "print(\"Total Number of training Dataset is:\",num_inputs_train)\n",
    "print(\"Dataset randomization and separation complete!\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-19T08:37:47.921611100Z",
     "start_time": "2023-06-19T08:37:47.869598Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/600\n",
      "949/949 [==============================] - 1s 926us/step - loss: 0.0017 - mse: 0.0017 - val_loss: 1.0464e-04 - val_mse: 1.0464e-04\n",
      "Epoch 2/600\n",
      "949/949 [==============================] - 1s 815us/step - loss: 6.0519e-05 - mse: 6.0519e-05 - val_loss: 4.6969e-05 - val_mse: 4.6969e-05\n",
      "Epoch 3/600\n",
      "949/949 [==============================] - 1s 806us/step - loss: 3.8788e-05 - mse: 3.8788e-05 - val_loss: 3.1125e-05 - val_mse: 3.1125e-05\n",
      "Epoch 4/600\n",
      "949/949 [==============================] - 1s 819us/step - loss: 2.8697e-05 - mse: 2.8697e-05 - val_loss: 3.1373e-05 - val_mse: 3.1373e-05\n",
      "Epoch 5/600\n",
      "949/949 [==============================] - 1s 823us/step - loss: 2.3016e-05 - mse: 2.3016e-05 - val_loss: 2.0607e-05 - val_mse: 2.0607e-05\n",
      "Epoch 6/600\n",
      "949/949 [==============================] - 1s 806us/step - loss: 1.7732e-05 - mse: 1.7732e-05 - val_loss: 1.5474e-05 - val_mse: 1.5474e-05\n",
      "Epoch 7/600\n",
      "949/949 [==============================] - 1s 810us/step - loss: 1.5092e-05 - mse: 1.5092e-05 - val_loss: 1.1712e-05 - val_mse: 1.1712e-05\n",
      "Epoch 8/600\n",
      "949/949 [==============================] - 1s 798us/step - loss: 1.0789e-05 - mse: 1.0789e-05 - val_loss: 1.0645e-05 - val_mse: 1.0645e-05\n",
      "Epoch 9/600\n",
      "949/949 [==============================] - 1s 815us/step - loss: 9.0331e-06 - mse: 9.0331e-06 - val_loss: 1.1555e-05 - val_mse: 1.1555e-05\n",
      "Epoch 10/600\n",
      "949/949 [==============================] - 1s 806us/step - loss: 6.6037e-06 - mse: 6.6037e-06 - val_loss: 4.6803e-06 - val_mse: 4.6803e-06\n",
      "Epoch 11/600\n",
      "949/949 [==============================] - 1s 802us/step - loss: 5.9722e-06 - mse: 5.9722e-06 - val_loss: 4.5388e-06 - val_mse: 4.5388e-06\n",
      "Epoch 12/600\n",
      "949/949 [==============================] - 1s 806us/step - loss: 5.7661e-06 - mse: 5.7661e-06 - val_loss: 1.1796e-05 - val_mse: 1.1796e-05\n",
      "Epoch 13/600\n",
      "949/949 [==============================] - 1s 810us/step - loss: 5.9310e-06 - mse: 5.9310e-06 - val_loss: 3.1473e-06 - val_mse: 3.1473e-06\n",
      "Epoch 14/600\n",
      "949/949 [==============================] - 1s 815us/step - loss: 5.3687e-06 - mse: 5.3687e-06 - val_loss: 2.9941e-06 - val_mse: 2.9941e-06\n",
      "Epoch 15/600\n",
      "949/949 [==============================] - 1s 806us/step - loss: 5.9208e-06 - mse: 5.9208e-06 - val_loss: 1.7934e-05 - val_mse: 1.7934e-05\n",
      "Epoch 16/600\n",
      "949/949 [==============================] - 1s 810us/step - loss: 5.5844e-06 - mse: 5.5844e-06 - val_loss: 2.0780e-06 - val_mse: 2.0780e-06\n",
      "Epoch 17/600\n",
      "949/949 [==============================] - 1s 810us/step - loss: 6.5839e-06 - mse: 6.5839e-06 - val_loss: 4.6077e-06 - val_mse: 4.6077e-06\n",
      "Epoch 18/600\n",
      "949/949 [==============================] - 1s 806us/step - loss: 4.6062e-06 - mse: 4.6062e-06 - val_loss: 8.3568e-06 - val_mse: 8.3568e-06\n",
      "Epoch 19/600\n",
      "949/949 [==============================] - 1s 810us/step - loss: 7.0191e-06 - mse: 7.0191e-06 - val_loss: 5.1192e-06 - val_mse: 5.1192e-06\n",
      "Epoch 20/600\n",
      "949/949 [==============================] - 1s 810us/step - loss: 3.4236e-06 - mse: 3.4236e-06 - val_loss: 2.6148e-06 - val_mse: 2.6148e-06\n",
      "Epoch 21/600\n",
      "949/949 [==============================] - 1s 819us/step - loss: 5.5378e-06 - mse: 5.5378e-06 - val_loss: 6.0931e-06 - val_mse: 6.0931e-06\n",
      "Epoch 22/600\n",
      "949/949 [==============================] - 1s 828us/step - loss: 4.9278e-06 - mse: 4.9278e-06 - val_loss: 1.3224e-06 - val_mse: 1.3224e-06\n",
      "Epoch 23/600\n",
      "949/949 [==============================] - 1s 823us/step - loss: 5.1557e-06 - mse: 5.1557e-06 - val_loss: 1.4182e-06 - val_mse: 1.4182e-06\n",
      "Epoch 24/600\n",
      "949/949 [==============================] - 1s 819us/step - loss: 5.3735e-06 - mse: 5.3735e-06 - val_loss: 6.1030e-07 - val_mse: 6.1030e-07\n",
      "Epoch 25/600\n",
      "949/949 [==============================] - 1s 810us/step - loss: 3.1750e-06 - mse: 3.1750e-06 - val_loss: 1.9856e-05 - val_mse: 1.9856e-05\n",
      "Epoch 26/600\n",
      "949/949 [==============================] - 1s 827us/step - loss: 3.5969e-06 - mse: 3.5969e-06 - val_loss: 3.6117e-07 - val_mse: 3.6117e-07\n",
      "Epoch 27/600\n",
      "949/949 [==============================] - 1s 819us/step - loss: 4.3549e-07 - mse: 4.3549e-07 - val_loss: 6.7717e-07 - val_mse: 6.7717e-07\n",
      "Epoch 28/600\n",
      "949/949 [==============================] - 1s 848us/step - loss: 1.3869e-06 - mse: 1.3869e-06 - val_loss: 5.4301e-07 - val_mse: 5.4301e-07\n",
      "Epoch 29/600\n",
      "949/949 [==============================] - 1s 823us/step - loss: 1.2508e-06 - mse: 1.2508e-06 - val_loss: 8.1199e-07 - val_mse: 8.1199e-07\n",
      "Epoch 30/600\n",
      "949/949 [==============================] - 1s 810us/step - loss: 1.0041e-06 - mse: 1.0041e-06 - val_loss: 2.6783e-06 - val_mse: 2.6783e-06\n",
      "Epoch 31/600\n",
      "949/949 [==============================] - 1s 810us/step - loss: 1.2327e-06 - mse: 1.2327e-06 - val_loss: 6.2306e-07 - val_mse: 6.2306e-07\n",
      "Epoch 32/600\n",
      "949/949 [==============================] - 1s 811us/step - loss: 1.1771e-06 - mse: 1.1771e-06 - val_loss: 4.1619e-07 - val_mse: 4.1619e-07\n",
      "Epoch 33/600\n",
      "949/949 [==============================] - 1s 815us/step - loss: 1.4047e-06 - mse: 1.4047e-06 - val_loss: 3.0283e-06 - val_mse: 3.0283e-06\n",
      "Epoch 34/600\n",
      "949/949 [==============================] - 1s 823us/step - loss: 1.2729e-06 - mse: 1.2729e-06 - val_loss: 2.6831e-07 - val_mse: 2.6831e-07\n",
      "Epoch 35/600\n",
      "949/949 [==============================] - 1s 818us/step - loss: 1.0405e-06 - mse: 1.0405e-06 - val_loss: 2.5342e-06 - val_mse: 2.5342e-06\n",
      "Epoch 36/600\n",
      "949/949 [==============================] - 1s 815us/step - loss: 1.1088e-06 - mse: 1.1088e-06 - val_loss: 1.2637e-06 - val_mse: 1.2637e-06\n",
      "Epoch 37/600\n",
      "949/949 [==============================] - 1s 810us/step - loss: 9.3923e-07 - mse: 9.3923e-07 - val_loss: 1.9666e-06 - val_mse: 1.9666e-06\n",
      "Epoch 38/600\n",
      "949/949 [==============================] - 1s 815us/step - loss: 1.4147e-06 - mse: 1.4147e-06 - val_loss: 1.0762e-06 - val_mse: 1.0762e-06\n",
      "Epoch 39/600\n",
      "949/949 [==============================] - 1s 802us/step - loss: 1.2925e-06 - mse: 1.2925e-06 - val_loss: 1.2314e-06 - val_mse: 1.2314e-06\n",
      "Epoch 40/600\n",
      "949/949 [==============================] - 1s 806us/step - loss: 1.3936e-06 - mse: 1.3936e-06 - val_loss: 1.7434e-06 - val_mse: 1.7434e-06\n",
      "Epoch 41/600\n",
      "949/949 [==============================] - 1s 810us/step - loss: 1.1138e-06 - mse: 1.1138e-06 - val_loss: 1.7156e-06 - val_mse: 1.7156e-06\n",
      "Epoch 42/600\n",
      "949/949 [==============================] - 1s 819us/step - loss: 9.4379e-07 - mse: 9.4379e-07 - val_loss: 7.5226e-06 - val_mse: 7.5226e-06\n",
      "Epoch 43/600\n",
      "949/949 [==============================] - 1s 798us/step - loss: 1.3508e-06 - mse: 1.3508e-06 - val_loss: 3.2867e-06 - val_mse: 3.2867e-06\n",
      "Epoch 44/600\n",
      "949/949 [==============================] - 1s 802us/step - loss: 8.9787e-07 - mse: 8.9787e-07 - val_loss: 1.3737e-06 - val_mse: 1.3737e-06\n",
      "Epoch 45/600\n",
      "949/949 [==============================] - 1s 815us/step - loss: 9.4925e-07 - mse: 9.4925e-07 - val_loss: 1.1624e-06 - val_mse: 1.1624e-06\n",
      "Epoch 46/600\n",
      "949/949 [==============================] - 1s 823us/step - loss: 1.0819e-06 - mse: 1.0819e-06 - val_loss: 1.9433e-06 - val_mse: 1.9433e-06\n",
      "Epoch 47/600\n",
      "949/949 [==============================] - 1s 819us/step - loss: 1.2516e-06 - mse: 1.2516e-06 - val_loss: 3.3095e-07 - val_mse: 3.3095e-07\n",
      "Epoch 48/600\n",
      "949/949 [==============================] - 1s 800us/step - loss: 9.9967e-07 - mse: 9.9967e-07 - val_loss: 3.7777e-07 - val_mse: 3.7777e-07\n",
      "Epoch 49/600\n",
      "949/949 [==============================] - 1s 827us/step - loss: 1.0100e-06 - mse: 1.0100e-06 - val_loss: 1.0171e-06 - val_mse: 1.0171e-06\n",
      "Epoch 50/600\n",
      "949/949 [==============================] - 1s 815us/step - loss: 1.3707e-06 - mse: 1.3707e-06 - val_loss: 8.1988e-07 - val_mse: 8.1988e-07\n",
      "Epoch 51/600\n",
      "949/949 [==============================] - 1s 806us/step - loss: 1.0551e-06 - mse: 1.0551e-06 - val_loss: 3.8070e-07 - val_mse: 3.8070e-07\n",
      "Epoch 52/600\n",
      "949/949 [==============================] - 1s 810us/step - loss: 4.4761e-07 - mse: 4.4761e-07 - val_loss: 7.5646e-08 - val_mse: 7.5646e-08\n",
      "Epoch 53/600\n",
      "949/949 [==============================] - 1s 812us/step - loss: 1.8440e-07 - mse: 1.8440e-07 - val_loss: 8.3055e-07 - val_mse: 8.3055e-07\n",
      "Epoch 54/600\n",
      "949/949 [==============================] - 1s 806us/step - loss: 3.5272e-07 - mse: 3.5272e-07 - val_loss: 1.9455e-07 - val_mse: 1.9455e-07\n",
      "Epoch 55/600\n",
      "949/949 [==============================] - 1s 806us/step - loss: 3.6729e-07 - mse: 3.6729e-07 - val_loss: 2.9785e-07 - val_mse: 2.9785e-07\n",
      "Epoch 56/600\n",
      "949/949 [==============================] - 1s 810us/step - loss: 3.1263e-07 - mse: 3.1263e-07 - val_loss: 1.0530e-07 - val_mse: 1.0530e-07\n",
      "Epoch 57/600\n",
      "949/949 [==============================] - 1s 802us/step - loss: 3.2920e-07 - mse: 3.2920e-07 - val_loss: 2.4463e-07 - val_mse: 2.4463e-07\n",
      "Epoch 58/600\n",
      "949/949 [==============================] - 1s 810us/step - loss: 3.7490e-07 - mse: 3.7490e-07 - val_loss: 1.7907e-07 - val_mse: 1.7907e-07\n",
      "Epoch 59/600\n",
      "949/949 [==============================] - 1s 802us/step - loss: 2.7407e-07 - mse: 2.7407e-07 - val_loss: 3.7277e-07 - val_mse: 3.7277e-07\n",
      "Epoch 60/600\n",
      "949/949 [==============================] - 1s 810us/step - loss: 3.0871e-07 - mse: 3.0871e-07 - val_loss: 2.0256e-07 - val_mse: 2.0256e-07\n",
      "Epoch 61/600\n",
      "949/949 [==============================] - 1s 806us/step - loss: 3.8518e-07 - mse: 3.8518e-07 - val_loss: 3.0169e-07 - val_mse: 3.0169e-07\n",
      "Epoch 62/600\n",
      "949/949 [==============================] - 1s 805us/step - loss: 3.5724e-07 - mse: 3.5724e-07 - val_loss: 1.7197e-07 - val_mse: 1.7197e-07\n",
      "Epoch 63/600\n",
      "949/949 [==============================] - 1s 811us/step - loss: 2.5112e-07 - mse: 2.5112e-07 - val_loss: 1.4246e-07 - val_mse: 1.4246e-07\n",
      "Epoch 64/600\n",
      "949/949 [==============================] - 1s 819us/step - loss: 5.1266e-07 - mse: 5.1266e-07 - val_loss: 7.0387e-08 - val_mse: 7.0387e-08\n",
      "Epoch 65/600\n",
      "949/949 [==============================] - 1s 815us/step - loss: 2.0700e-07 - mse: 2.0700e-07 - val_loss: 7.7983e-07 - val_mse: 7.7983e-07\n",
      "Epoch 66/600\n",
      "949/949 [==============================] - 1s 815us/step - loss: 2.9001e-07 - mse: 2.9001e-07 - val_loss: 1.1819e-06 - val_mse: 1.1819e-06\n",
      "Epoch 67/600\n",
      "949/949 [==============================] - 1s 810us/step - loss: 2.8954e-07 - mse: 2.8954e-07 - val_loss: 3.2966e-07 - val_mse: 3.2966e-07\n",
      "Epoch 68/600\n",
      "949/949 [==============================] - 1s 815us/step - loss: 3.1400e-07 - mse: 3.1400e-07 - val_loss: 1.9075e-07 - val_mse: 1.9075e-07\n",
      "Epoch 69/600\n",
      "949/949 [==============================] - 1s 819us/step - loss: 3.3201e-07 - mse: 3.3201e-07 - val_loss: 4.0983e-07 - val_mse: 4.0983e-07\n",
      "Epoch 70/600\n",
      "949/949 [==============================] - 1s 819us/step - loss: 2.8755e-07 - mse: 2.8755e-07 - val_loss: 5.3828e-07 - val_mse: 5.3828e-07\n",
      "Epoch 71/600\n",
      "949/949 [==============================] - 1s 834us/step - loss: 3.0361e-07 - mse: 3.0361e-07 - val_loss: 1.8523e-07 - val_mse: 1.8523e-07\n",
      "Epoch 72/600\n",
      "949/949 [==============================] - 1s 815us/step - loss: 3.5202e-07 - mse: 3.5202e-07 - val_loss: 6.7462e-07 - val_mse: 6.7462e-07\n",
      "Epoch 73/600\n",
      "949/949 [==============================] - 1s 827us/step - loss: 2.3847e-07 - mse: 2.3847e-07 - val_loss: 9.9811e-08 - val_mse: 9.9811e-08\n",
      "Epoch 74/600\n",
      "949/949 [==============================] - 1s 827us/step - loss: 2.6316e-07 - mse: 2.6316e-07 - val_loss: 9.7638e-08 - val_mse: 9.7638e-08\n",
      "Epoch 75/600\n",
      "949/949 [==============================] - 1s 815us/step - loss: 3.5661e-07 - mse: 3.5661e-07 - val_loss: 8.8460e-08 - val_mse: 8.8460e-08\n",
      "Epoch 76/600\n",
      "949/949 [==============================] - 1s 819us/step - loss: 2.8635e-07 - mse: 2.8635e-07 - val_loss: 3.2208e-06 - val_mse: 3.2208e-06\n",
      "Epoch 77/600\n",
      "949/949 [==============================] - 1s 827us/step - loss: 3.3395e-07 - mse: 3.3395e-07 - val_loss: 1.0183e-06 - val_mse: 1.0183e-06\n",
      "Epoch 78/600\n",
      "949/949 [==============================] - 1s 823us/step - loss: 8.1447e-08 - mse: 8.1447e-08 - val_loss: 3.5220e-08 - val_mse: 3.5220e-08\n",
      "Epoch 79/600\n",
      "949/949 [==============================] - 1s 818us/step - loss: 6.6301e-08 - mse: 6.6301e-08 - val_loss: 5.0608e-08 - val_mse: 5.0608e-08\n",
      "Epoch 80/600\n",
      "949/949 [==============================] - 1s 815us/step - loss: 1.0986e-07 - mse: 1.0986e-07 - val_loss: 1.2343e-07 - val_mse: 1.2343e-07\n",
      "Epoch 81/600\n",
      "949/949 [==============================] - 1s 823us/step - loss: 1.0050e-07 - mse: 1.0050e-07 - val_loss: 8.3221e-08 - val_mse: 8.3221e-08\n",
      "Epoch 82/600\n",
      "949/949 [==============================] - 1s 810us/step - loss: 1.0148e-07 - mse: 1.0148e-07 - val_loss: 4.7004e-08 - val_mse: 4.7004e-08\n",
      "Epoch 83/600\n",
      "949/949 [==============================] - 1s 819us/step - loss: 8.7281e-08 - mse: 8.7281e-08 - val_loss: 6.1970e-08 - val_mse: 6.1970e-08\n",
      "Epoch 84/600\n",
      "949/949 [==============================] - 1s 823us/step - loss: 8.0089e-08 - mse: 8.0089e-08 - val_loss: 5.8366e-08 - val_mse: 5.8366e-08\n",
      "Epoch 85/600\n",
      "949/949 [==============================] - 1s 819us/step - loss: 1.0953e-07 - mse: 1.0953e-07 - val_loss: 6.2162e-08 - val_mse: 6.2162e-08\n",
      "Epoch 86/600\n",
      "949/949 [==============================] - 1s 810us/step - loss: 9.8072e-08 - mse: 9.8072e-08 - val_loss: 1.2562e-07 - val_mse: 1.2562e-07\n",
      "Epoch 87/600\n",
      "949/949 [==============================] - 1s 798us/step - loss: 1.1338e-07 - mse: 1.1338e-07 - val_loss: 2.1480e-07 - val_mse: 2.1480e-07\n",
      "Epoch 88/600\n",
      "949/949 [==============================] - 1s 789us/step - loss: 7.4487e-08 - mse: 7.4487e-08 - val_loss: 1.8744e-07 - val_mse: 1.8744e-07\n",
      "Epoch 89/600\n",
      "949/949 [==============================] - 1s 793us/step - loss: 1.0527e-07 - mse: 1.0527e-07 - val_loss: 7.4418e-08 - val_mse: 7.4418e-08\n",
      "Epoch 90/600\n",
      "949/949 [==============================] - 1s 793us/step - loss: 9.0790e-08 - mse: 9.0790e-08 - val_loss: 9.5118e-08 - val_mse: 9.5118e-08\n",
      "Epoch 91/600\n",
      "949/949 [==============================] - 1s 785us/step - loss: 9.7801e-08 - mse: 9.7801e-08 - val_loss: 4.6620e-08 - val_mse: 4.6620e-08\n",
      "Epoch 92/600\n",
      "949/949 [==============================] - 1s 789us/step - loss: 8.8708e-08 - mse: 8.8708e-08 - val_loss: 4.2202e-07 - val_mse: 4.2202e-07\n",
      "Epoch 93/600\n",
      "949/949 [==============================] - 1s 789us/step - loss: 1.0142e-07 - mse: 1.0142e-07 - val_loss: 1.9541e-07 - val_mse: 1.9541e-07\n",
      "Epoch 94/600\n",
      "949/949 [==============================] - 1s 802us/step - loss: 1.1819e-07 - mse: 1.1819e-07 - val_loss: 1.8136e-07 - val_mse: 1.8136e-07\n",
      "Epoch 95/600\n",
      "949/949 [==============================] - 1s 797us/step - loss: 9.0284e-08 - mse: 9.0284e-08 - val_loss: 5.9011e-08 - val_mse: 5.9011e-08\n",
      "Epoch 96/600\n",
      "949/949 [==============================] - 1s 798us/step - loss: 9.8098e-08 - mse: 9.8098e-08 - val_loss: 8.4816e-08 - val_mse: 8.4816e-08\n",
      "Epoch 97/600\n",
      "949/949 [==============================] - 1s 793us/step - loss: 8.0096e-08 - mse: 8.0096e-08 - val_loss: 9.2207e-08 - val_mse: 9.2207e-08\n",
      "Epoch 98/600\n",
      "949/949 [==============================] - 1s 798us/step - loss: 8.3720e-08 - mse: 8.3720e-08 - val_loss: 8.2209e-08 - val_mse: 8.2209e-08\n",
      "Epoch 99/600\n",
      "949/949 [==============================] - 1s 798us/step - loss: 9.8395e-08 - mse: 9.8395e-08 - val_loss: 1.3397e-07 - val_mse: 1.3397e-07\n",
      "Epoch 100/600\n",
      "949/949 [==============================] - 1s 798us/step - loss: 1.0029e-07 - mse: 1.0029e-07 - val_loss: 9.5334e-08 - val_mse: 9.5334e-08\n",
      "Epoch 101/600\n",
      "949/949 [==============================] - 1s 802us/step - loss: 7.2081e-08 - mse: 7.2081e-08 - val_loss: 1.4219e-07 - val_mse: 1.4219e-07\n",
      "Epoch 102/600\n",
      "949/949 [==============================] - 1s 789us/step - loss: 9.3283e-08 - mse: 9.3283e-08 - val_loss: 2.2365e-07 - val_mse: 2.2365e-07\n",
      "Epoch 103/600\n",
      "949/949 [==============================] - 1s 796us/step - loss: 7.5766e-08 - mse: 7.5766e-08 - val_loss: 2.3816e-08 - val_mse: 2.3816e-08\n",
      "Epoch 104/600\n",
      "949/949 [==============================] - 1s 802us/step - loss: 2.4683e-08 - mse: 2.4683e-08 - val_loss: 3.6581e-08 - val_mse: 3.6581e-08\n",
      "Epoch 105/600\n",
      "949/949 [==============================] - 1s 798us/step - loss: 3.3377e-08 - mse: 3.3377e-08 - val_loss: 2.7802e-08 - val_mse: 2.7802e-08\n",
      "Epoch 106/600\n",
      "949/949 [==============================] - 1s 802us/step - loss: 3.4521e-08 - mse: 3.4521e-08 - val_loss: 2.8225e-08 - val_mse: 2.8225e-08\n",
      "Epoch 107/600\n",
      "949/949 [==============================] - 1s 798us/step - loss: 3.7669e-08 - mse: 3.7669e-08 - val_loss: 6.4743e-08 - val_mse: 6.4743e-08\n",
      "Epoch 108/600\n",
      "949/949 [==============================] - 1s 798us/step - loss: 3.7087e-08 - mse: 3.7087e-08 - val_loss: 2.8686e-08 - val_mse: 2.8686e-08\n",
      "Epoch 109/600\n",
      "949/949 [==============================] - 1s 798us/step - loss: 3.7021e-08 - mse: 3.7021e-08 - val_loss: 3.3524e-08 - val_mse: 3.3524e-08\n",
      "Epoch 110/600\n",
      "949/949 [==============================] - 1s 814us/step - loss: 3.4998e-08 - mse: 3.4998e-08 - val_loss: 2.6443e-08 - val_mse: 2.6443e-08\n",
      "Epoch 111/600\n",
      "949/949 [==============================] - 1s 806us/step - loss: 3.4422e-08 - mse: 3.4422e-08 - val_loss: 3.9432e-08 - val_mse: 3.9432e-08\n",
      "Epoch 112/600\n",
      "949/949 [==============================] - 1s 815us/step - loss: 3.3010e-08 - mse: 3.3010e-08 - val_loss: 4.5028e-08 - val_mse: 4.5028e-08\n",
      "Epoch 113/600\n",
      "949/949 [==============================] - 1s 802us/step - loss: 3.8763e-08 - mse: 3.8763e-08 - val_loss: 2.5253e-08 - val_mse: 2.5253e-08\n",
      "Epoch 114/600\n",
      "949/949 [==============================] - 1s 802us/step - loss: 3.2396e-08 - mse: 3.2396e-08 - val_loss: 3.4338e-08 - val_mse: 3.4338e-08\n",
      "Epoch 115/600\n",
      "949/949 [==============================] - 1s 798us/step - loss: 3.0261e-08 - mse: 3.0261e-08 - val_loss: 2.6469e-08 - val_mse: 2.6469e-08\n",
      "Epoch 116/600\n",
      "949/949 [==============================] - 1s 802us/step - loss: 3.3382e-08 - mse: 3.3382e-08 - val_loss: 2.2300e-08 - val_mse: 2.2300e-08\n",
      "Epoch 117/600\n",
      "949/949 [==============================] - 1s 814us/step - loss: 3.2446e-08 - mse: 3.2446e-08 - val_loss: 4.9212e-08 - val_mse: 4.9212e-08\n",
      "Epoch 118/600\n",
      "949/949 [==============================] - 1s 805us/step - loss: 3.5231e-08 - mse: 3.5231e-08 - val_loss: 2.6191e-08 - val_mse: 2.6191e-08\n",
      "Epoch 119/600\n",
      "949/949 [==============================] - 1s 815us/step - loss: 3.3108e-08 - mse: 3.3108e-08 - val_loss: 2.2602e-08 - val_mse: 2.2602e-08\n",
      "Epoch 120/600\n",
      "949/949 [==============================] - 1s 812us/step - loss: 3.7931e-08 - mse: 3.7931e-08 - val_loss: 4.4576e-08 - val_mse: 4.4576e-08\n",
      "Epoch 121/600\n",
      "949/949 [==============================] - 1s 810us/step - loss: 2.8388e-08 - mse: 2.8388e-08 - val_loss: 2.2029e-08 - val_mse: 2.2029e-08\n",
      "Epoch 122/600\n",
      "949/949 [==============================] - 1s 819us/step - loss: 3.4620e-08 - mse: 3.4620e-08 - val_loss: 2.8867e-08 - val_mse: 2.8867e-08\n",
      "Epoch 123/600\n",
      "949/949 [==============================] - 1s 806us/step - loss: 3.4350e-08 - mse: 3.4350e-08 - val_loss: 4.2061e-08 - val_mse: 4.2061e-08\n",
      "Epoch 124/600\n",
      "949/949 [==============================] - 1s 810us/step - loss: 3.1181e-08 - mse: 3.1181e-08 - val_loss: 1.9289e-08 - val_mse: 1.9289e-08\n",
      "Epoch 125/600\n",
      "949/949 [==============================] - 1s 807us/step - loss: 3.3638e-08 - mse: 3.3638e-08 - val_loss: 3.2802e-08 - val_mse: 3.2802e-08\n",
      "Epoch 126/600\n",
      "949/949 [==============================] - 1s 819us/step - loss: 3.2901e-08 - mse: 3.2901e-08 - val_loss: 5.1801e-08 - val_mse: 5.1801e-08\n",
      "Epoch 127/600\n",
      "949/949 [==============================] - 1s 815us/step - loss: 3.5481e-08 - mse: 3.5481e-08 - val_loss: 3.4814e-08 - val_mse: 3.4814e-08\n",
      "Epoch 128/600\n",
      "949/949 [==============================] - 1s 815us/step - loss: 2.9093e-08 - mse: 2.9093e-08 - val_loss: 3.4935e-08 - val_mse: 3.4935e-08\n",
      "Epoch 129/600\n",
      "949/949 [==============================] - 1s 815us/step - loss: 2.0806e-08 - mse: 2.0806e-08 - val_loss: 1.1377e-08 - val_mse: 1.1377e-08\n",
      "Epoch 130/600\n",
      "949/949 [==============================] - 1s 815us/step - loss: 1.4795e-08 - mse: 1.4795e-08 - val_loss: 1.8137e-08 - val_mse: 1.8137e-08\n",
      "Epoch 131/600\n",
      "949/949 [==============================] - 1s 810us/step - loss: 1.7148e-08 - mse: 1.7148e-08 - val_loss: 1.7132e-08 - val_mse: 1.7132e-08\n",
      "Epoch 132/600\n",
      "949/949 [==============================] - 1s 815us/step - loss: 1.7314e-08 - mse: 1.7314e-08 - val_loss: 1.7913e-08 - val_mse: 1.7913e-08\n",
      "Epoch 133/600\n",
      "949/949 [==============================] - 1s 815us/step - loss: 1.7558e-08 - mse: 1.7558e-08 - val_loss: 2.1945e-08 - val_mse: 2.1945e-08\n",
      "Epoch 134/600\n",
      "949/949 [==============================] - 1s 810us/step - loss: 1.6872e-08 - mse: 1.6872e-08 - val_loss: 3.0213e-08 - val_mse: 3.0213e-08\n",
      "Epoch 135/600\n",
      "949/949 [==============================] - 1s 819us/step - loss: 1.7485e-08 - mse: 1.7485e-08 - val_loss: 1.7879e-08 - val_mse: 1.7879e-08\n",
      "Epoch 136/600\n",
      "949/949 [==============================] - 1s 815us/step - loss: 1.6532e-08 - mse: 1.6532e-08 - val_loss: 1.6617e-08 - val_mse: 1.6617e-08\n",
      "Epoch 137/600\n",
      "949/949 [==============================] - 1s 815us/step - loss: 1.6513e-08 - mse: 1.6513e-08 - val_loss: 2.1883e-08 - val_mse: 2.1883e-08\n",
      "Epoch 138/600\n",
      "949/949 [==============================] - 1s 827us/step - loss: 1.6741e-08 - mse: 1.6741e-08 - val_loss: 2.5162e-08 - val_mse: 2.5162e-08\n",
      "Epoch 139/600\n",
      "949/949 [==============================] - 1s 810us/step - loss: 1.7548e-08 - mse: 1.7548e-08 - val_loss: 1.5005e-08 - val_mse: 1.5005e-08\n",
      "Epoch 140/600\n",
      "949/949 [==============================] - 1s 806us/step - loss: 1.7103e-08 - mse: 1.7103e-08 - val_loss: 2.0483e-08 - val_mse: 2.0483e-08\n",
      "Epoch 141/600\n",
      "949/949 [==============================] - 1s 798us/step - loss: 1.6011e-08 - mse: 1.6011e-08 - val_loss: 1.2716e-08 - val_mse: 1.2716e-08\n",
      "Epoch 142/600\n",
      "949/949 [==============================] - 1s 798us/step - loss: 1.7048e-08 - mse: 1.7048e-08 - val_loss: 1.2355e-08 - val_mse: 1.2355e-08\n",
      "Epoch 143/600\n",
      "949/949 [==============================] - 1s 823us/step - loss: 1.6029e-08 - mse: 1.6029e-08 - val_loss: 1.6501e-08 - val_mse: 1.6501e-08\n",
      "Epoch 144/600\n",
      "949/949 [==============================] - 1s 806us/step - loss: 1.6592e-08 - mse: 1.6592e-08 - val_loss: 1.1423e-08 - val_mse: 1.1423e-08\n",
      "Epoch 145/600\n",
      "949/949 [==============================] - 1s 806us/step - loss: 1.6163e-08 - mse: 1.6163e-08 - val_loss: 1.3606e-08 - val_mse: 1.3606e-08\n",
      "Epoch 146/600\n",
      "949/949 [==============================] - 1s 801us/step - loss: 1.6523e-08 - mse: 1.6523e-08 - val_loss: 1.2637e-08 - val_mse: 1.2637e-08\n",
      "Epoch 147/600\n",
      "949/949 [==============================] - 1s 802us/step - loss: 1.5772e-08 - mse: 1.5772e-08 - val_loss: 2.0124e-08 - val_mse: 2.0124e-08\n",
      "Epoch 148/600\n",
      "949/949 [==============================] - 1s 810us/step - loss: 1.5710e-08 - mse: 1.5710e-08 - val_loss: 2.1108e-08 - val_mse: 2.1108e-08\n",
      "Epoch 149/600\n",
      "949/949 [==============================] - 1s 815us/step - loss: 1.6159e-08 - mse: 1.6159e-08 - val_loss: 3.8233e-08 - val_mse: 3.8233e-08\n",
      "Epoch 150/600\n",
      "949/949 [==============================] - 1s 815us/step - loss: 1.5842e-08 - mse: 1.5842e-08 - val_loss: 2.0138e-08 - val_mse: 2.0138e-08\n",
      "Epoch 151/600\n",
      "949/949 [==============================] - 1s 819us/step - loss: 1.6143e-08 - mse: 1.6143e-08 - val_loss: 1.5976e-08 - val_mse: 1.5976e-08\n",
      "Epoch 152/600\n",
      "949/949 [==============================] - 1s 814us/step - loss: 1.5939e-08 - mse: 1.5939e-08 - val_loss: 2.9155e-08 - val_mse: 2.9155e-08\n",
      "Epoch 153/600\n",
      "949/949 [==============================] - 1s 815us/step - loss: 1.5073e-08 - mse: 1.5073e-08 - val_loss: 1.8608e-08 - val_mse: 1.8608e-08\n",
      "Epoch 154/600\n",
      "949/949 [==============================] - 1s 836us/step - loss: 1.6010e-08 - mse: 1.6010e-08 - val_loss: 1.9792e-08 - val_mse: 1.9792e-08\n",
      "Epoch 155/600\n",
      "949/949 [==============================] - 1s 832us/step - loss: 9.5518e-09 - mse: 9.5518e-09 - val_loss: 9.2456e-09 - val_mse: 9.2456e-09\n",
      "Epoch 156/600\n",
      "949/949 [==============================] - 1s 829us/step - loss: 1.0462e-08 - mse: 1.0462e-08 - val_loss: 9.5445e-09 - val_mse: 9.5445e-09\n",
      "Epoch 157/600\n",
      "949/949 [==============================] - 1s 823us/step - loss: 1.0544e-08 - mse: 1.0544e-08 - val_loss: 1.3481e-08 - val_mse: 1.3481e-08\n",
      "Epoch 158/600\n",
      "949/949 [==============================] - 1s 827us/step - loss: 1.0353e-08 - mse: 1.0353e-08 - val_loss: 9.9866e-09 - val_mse: 9.9866e-09\n",
      "Epoch 159/600\n",
      "949/949 [==============================] - 1s 826us/step - loss: 1.0512e-08 - mse: 1.0512e-08 - val_loss: 9.0174e-09 - val_mse: 9.0174e-09\n",
      "Epoch 160/600\n",
      "949/949 [==============================] - 1s 831us/step - loss: 1.0478e-08 - mse: 1.0478e-08 - val_loss: 1.3162e-08 - val_mse: 1.3162e-08\n",
      "Epoch 161/600\n",
      "949/949 [==============================] - 1s 823us/step - loss: 1.0551e-08 - mse: 1.0551e-08 - val_loss: 1.3662e-08 - val_mse: 1.3662e-08\n",
      "Epoch 162/600\n",
      "949/949 [==============================] - 1s 823us/step - loss: 1.0483e-08 - mse: 1.0483e-08 - val_loss: 1.1272e-08 - val_mse: 1.1272e-08\n",
      "Epoch 163/600\n",
      "949/949 [==============================] - 1s 819us/step - loss: 1.0469e-08 - mse: 1.0469e-08 - val_loss: 1.3270e-08 - val_mse: 1.3270e-08\n",
      "Epoch 164/600\n",
      "949/949 [==============================] - 1s 830us/step - loss: 9.9198e-09 - mse: 9.9198e-09 - val_loss: 1.4662e-08 - val_mse: 1.4662e-08\n",
      "Epoch 165/600\n",
      "949/949 [==============================] - 1s 824us/step - loss: 1.0576e-08 - mse: 1.0576e-08 - val_loss: 1.0021e-08 - val_mse: 1.0021e-08\n",
      "Epoch 166/600\n",
      "949/949 [==============================] - 1s 815us/step - loss: 1.0303e-08 - mse: 1.0303e-08 - val_loss: 8.2903e-09 - val_mse: 8.2903e-09\n",
      "Epoch 167/600\n",
      "949/949 [==============================] - 1s 815us/step - loss: 1.0278e-08 - mse: 1.0278e-08 - val_loss: 8.6169e-09 - val_mse: 8.6169e-09\n",
      "Epoch 168/600\n",
      "949/949 [==============================] - 1s 819us/step - loss: 1.0190e-08 - mse: 1.0190e-08 - val_loss: 1.6764e-08 - val_mse: 1.6764e-08\n",
      "Epoch 169/600\n",
      "949/949 [==============================] - 1s 815us/step - loss: 1.0582e-08 - mse: 1.0582e-08 - val_loss: 1.3384e-08 - val_mse: 1.3384e-08\n",
      "Epoch 170/600\n",
      "949/949 [==============================] - 1s 844us/step - loss: 9.9255e-09 - mse: 9.9255e-09 - val_loss: 1.0527e-08 - val_mse: 1.0527e-08\n",
      "Epoch 171/600\n",
      "949/949 [==============================] - 1s 840us/step - loss: 1.0324e-08 - mse: 1.0324e-08 - val_loss: 8.3403e-09 - val_mse: 8.3403e-09\n",
      "Epoch 172/600\n",
      "949/949 [==============================] - 1s 812us/step - loss: 1.0317e-08 - mse: 1.0317e-08 - val_loss: 9.1532e-09 - val_mse: 9.1532e-09\n",
      "Epoch 173/600\n",
      "949/949 [==============================] - 1s 815us/step - loss: 9.8751e-09 - mse: 9.8751e-09 - val_loss: 1.2226e-08 - val_mse: 1.2226e-08\n",
      "Epoch 174/600\n",
      "949/949 [==============================] - 1s 819us/step - loss: 9.9892e-09 - mse: 9.9892e-09 - val_loss: 9.5450e-09 - val_mse: 9.5450e-09\n",
      "Epoch 175/600\n",
      "949/949 [==============================] - 1s 819us/step - loss: 9.9076e-09 - mse: 9.9076e-09 - val_loss: 1.2907e-08 - val_mse: 1.2907e-08\n",
      "Epoch 176/600\n",
      "949/949 [==============================] - 1s 815us/step - loss: 9.9457e-09 - mse: 9.9457e-09 - val_loss: 1.4584e-08 - val_mse: 1.4584e-08\n",
      "Epoch 177/600\n",
      "949/949 [==============================] - 1s 819us/step - loss: 1.0384e-08 - mse: 1.0384e-08 - val_loss: 1.4885e-08 - val_mse: 1.4885e-08\n",
      "Epoch 178/600\n",
      "949/949 [==============================] - 1s 825us/step - loss: 9.9812e-09 - mse: 9.9812e-09 - val_loss: 1.2707e-08 - val_mse: 1.2707e-08\n",
      "Epoch 179/600\n",
      "949/949 [==============================] - 1s 836us/step - loss: 9.8670e-09 - mse: 9.8670e-09 - val_loss: 1.3313e-08 - val_mse: 1.3313e-08\n",
      "Epoch 180/600\n",
      "949/949 [==============================] - 1s 819us/step - loss: 9.6521e-09 - mse: 9.6521e-09 - val_loss: 8.8564e-09 - val_mse: 8.8564e-09\n",
      "Epoch 181/600\n",
      "949/949 [==============================] - 1s 815us/step - loss: 7.2582e-09 - mse: 7.2582e-09 - val_loss: 7.2211e-09 - val_mse: 7.2211e-09\n",
      "Epoch 182/600\n",
      "949/949 [==============================] - 1s 827us/step - loss: 7.6358e-09 - mse: 7.6358e-09 - val_loss: 8.7081e-09 - val_mse: 8.7081e-09\n",
      "Epoch 183/600\n",
      "949/949 [==============================] - 1s 844us/step - loss: 7.7643e-09 - mse: 7.7643e-09 - val_loss: 8.2811e-09 - val_mse: 8.2811e-09\n",
      "Epoch 184/600\n",
      "949/949 [==============================] - 1s 821us/step - loss: 7.6475e-09 - mse: 7.6475e-09 - val_loss: 6.8584e-09 - val_mse: 6.8584e-09\n",
      "Epoch 185/600\n",
      "949/949 [==============================] - 1s 815us/step - loss: 7.8257e-09 - mse: 7.8257e-09 - val_loss: 1.0947e-08 - val_mse: 1.0947e-08\n",
      "Epoch 186/600\n",
      "949/949 [==============================] - 1s 819us/step - loss: 7.8164e-09 - mse: 7.8164e-09 - val_loss: 1.2211e-08 - val_mse: 1.2211e-08\n",
      "Epoch 187/600\n",
      "949/949 [==============================] - 1s 815us/step - loss: 7.5652e-09 - mse: 7.5652e-09 - val_loss: 9.9724e-09 - val_mse: 9.9724e-09\n",
      "Epoch 188/600\n",
      "949/949 [==============================] - 1s 823us/step - loss: 7.7657e-09 - mse: 7.7657e-09 - val_loss: 7.8930e-09 - val_mse: 7.8930e-09\n",
      "Epoch 189/600\n",
      "949/949 [==============================] - 1s 810us/step - loss: 7.5168e-09 - mse: 7.5168e-09 - val_loss: 7.5920e-09 - val_mse: 7.5920e-09\n",
      "Epoch 190/600\n",
      "949/949 [==============================] - 1s 818us/step - loss: 7.7202e-09 - mse: 7.7202e-09 - val_loss: 6.0313e-09 - val_mse: 6.0313e-09\n",
      "Epoch 191/600\n",
      "949/949 [==============================] - 1s 811us/step - loss: 7.5000e-09 - mse: 7.5000e-09 - val_loss: 7.3450e-09 - val_mse: 7.3450e-09\n",
      "Epoch 192/600\n",
      "949/949 [==============================] - 1s 806us/step - loss: 7.5893e-09 - mse: 7.5893e-09 - val_loss: 1.0980e-08 - val_mse: 1.0980e-08\n",
      "Epoch 193/600\n",
      "949/949 [==============================] - 1s 814us/step - loss: 7.6402e-09 - mse: 7.6402e-09 - val_loss: 6.8883e-09 - val_mse: 6.8883e-09\n",
      "Epoch 194/600\n",
      "949/949 [==============================] - 1s 815us/step - loss: 7.6614e-09 - mse: 7.6614e-09 - val_loss: 6.6637e-09 - val_mse: 6.6637e-09\n",
      "Epoch 195/600\n",
      "949/949 [==============================] - 1s 823us/step - loss: 7.4642e-09 - mse: 7.4642e-09 - val_loss: 7.8286e-09 - val_mse: 7.8286e-09\n",
      "Epoch 196/600\n",
      "949/949 [==============================] - 1s 814us/step - loss: 7.5569e-09 - mse: 7.5569e-09 - val_loss: 8.9630e-09 - val_mse: 8.9630e-09\n",
      "Epoch 197/600\n",
      "949/949 [==============================] - 1s 815us/step - loss: 7.5380e-09 - mse: 7.5380e-09 - val_loss: 9.6793e-09 - val_mse: 9.6793e-09\n",
      "Epoch 198/600\n",
      "949/949 [==============================] - 1s 815us/step - loss: 7.4419e-09 - mse: 7.4419e-09 - val_loss: 9.4886e-09 - val_mse: 9.4886e-09\n",
      "Epoch 199/600\n",
      "949/949 [==============================] - 1s 819us/step - loss: 7.4526e-09 - mse: 7.4526e-09 - val_loss: 9.8306e-09 - val_mse: 9.8306e-09\n",
      "Epoch 200/600\n",
      "949/949 [==============================] - 1s 819us/step - loss: 7.4921e-09 - mse: 7.4921e-09 - val_loss: 7.5903e-09 - val_mse: 7.5903e-09\n",
      "Epoch 201/600\n",
      "949/949 [==============================] - 1s 823us/step - loss: 7.3502e-09 - mse: 7.3502e-09 - val_loss: 9.0649e-09 - val_mse: 9.0649e-09\n",
      "Epoch 202/600\n",
      "949/949 [==============================] - 1s 815us/step - loss: 7.5354e-09 - mse: 7.5354e-09 - val_loss: 6.9674e-09 - val_mse: 6.9674e-09\n",
      "Epoch 203/600\n",
      "949/949 [==============================] - 1s 815us/step - loss: 7.3591e-09 - mse: 7.3591e-09 - val_loss: 8.5158e-09 - val_mse: 8.5158e-09\n",
      "Epoch 204/600\n",
      "949/949 [==============================] - 1s 815us/step - loss: 7.4879e-09 - mse: 7.4879e-09 - val_loss: 7.0942e-09 - val_mse: 7.0942e-09\n",
      "Epoch 205/600\n",
      "949/949 [==============================] - 1s 823us/step - loss: 7.5223e-09 - mse: 7.5223e-09 - val_loss: 6.8767e-09 - val_mse: 6.8767e-09\n",
      "Epoch 206/600\n",
      "949/949 [==============================] - 1s 827us/step - loss: 6.7526e-09 - mse: 6.7526e-09 - val_loss: 6.2751e-09 - val_mse: 6.2751e-09\n",
      "Epoch 207/600\n",
      "949/949 [==============================] - 1s 830us/step - loss: 6.2094e-09 - mse: 6.2094e-09 - val_loss: 6.7238e-09 - val_mse: 6.7238e-09\n",
      "Epoch 208/600\n",
      "949/949 [==============================] - 1s 810us/step - loss: 6.3737e-09 - mse: 6.3737e-09 - val_loss: 7.2317e-09 - val_mse: 7.2317e-09\n",
      "Epoch 209/600\n",
      "949/949 [==============================] - 1s 805us/step - loss: 6.3081e-09 - mse: 6.3081e-09 - val_loss: 6.6099e-09 - val_mse: 6.6099e-09\n",
      "Epoch 210/600\n",
      "949/949 [==============================] - 1s 827us/step - loss: 6.4600e-09 - mse: 6.4600e-09 - val_loss: 5.4044e-09 - val_mse: 5.4044e-09\n",
      "Epoch 211/600\n",
      "949/949 [==============================] - 1s 823us/step - loss: 6.2526e-09 - mse: 6.2526e-09 - val_loss: 7.2327e-09 - val_mse: 7.2327e-09\n",
      "Epoch 212/600\n",
      "949/949 [==============================] - 1s 823us/step - loss: 6.3971e-09 - mse: 6.3971e-09 - val_loss: 6.0896e-09 - val_mse: 6.0896e-09\n",
      "Epoch 213/600\n",
      "949/949 [==============================] - 1s 817us/step - loss: 6.3799e-09 - mse: 6.3799e-09 - val_loss: 7.7604e-09 - val_mse: 7.7604e-09\n",
      "Epoch 214/600\n",
      "949/949 [==============================] - 1s 810us/step - loss: 6.2633e-09 - mse: 6.2633e-09 - val_loss: 6.5524e-09 - val_mse: 6.5524e-09\n",
      "Epoch 215/600\n",
      "949/949 [==============================] - 1s 819us/step - loss: 6.3878e-09 - mse: 6.3878e-09 - val_loss: 5.9840e-09 - val_mse: 5.9840e-09\n",
      "Epoch 216/600\n",
      "949/949 [==============================] - 1s 819us/step - loss: 6.3499e-09 - mse: 6.3499e-09 - val_loss: 6.7696e-09 - val_mse: 6.7696e-09\n",
      "Epoch 217/600\n",
      "949/949 [==============================] - 1s 814us/step - loss: 6.3287e-09 - mse: 6.3287e-09 - val_loss: 7.0976e-09 - val_mse: 7.0976e-09\n",
      "Epoch 218/600\n",
      "949/949 [==============================] - 1s 823us/step - loss: 6.2032e-09 - mse: 6.2032e-09 - val_loss: 6.8523e-09 - val_mse: 6.8523e-09\n",
      "Epoch 219/600\n",
      "949/949 [==============================] - 1s 806us/step - loss: 6.2354e-09 - mse: 6.2354e-09 - val_loss: 6.9623e-09 - val_mse: 6.9623e-09\n",
      "Epoch 220/600\n",
      "949/949 [==============================] - 1s 819us/step - loss: 6.2701e-09 - mse: 6.2701e-09 - val_loss: 6.7036e-09 - val_mse: 6.7036e-09\n",
      "Epoch 221/600\n",
      "949/949 [==============================] - 1s 827us/step - loss: 6.3148e-09 - mse: 6.3148e-09 - val_loss: 6.8485e-09 - val_mse: 6.8485e-09\n",
      "Epoch 222/600\n",
      "949/949 [==============================] - 1s 806us/step - loss: 6.2609e-09 - mse: 6.2609e-09 - val_loss: 5.8869e-09 - val_mse: 5.8869e-09\n",
      "Epoch 223/600\n",
      "949/949 [==============================] - 1s 827us/step - loss: 6.3294e-09 - mse: 6.3294e-09 - val_loss: 6.1960e-09 - val_mse: 6.1960e-09\n",
      "Epoch 224/600\n",
      "949/949 [==============================] - 1s 824us/step - loss: 6.3115e-09 - mse: 6.3115e-09 - val_loss: 6.0851e-09 - val_mse: 6.0851e-09\n",
      "Epoch 225/600\n",
      "949/949 [==============================] - 1s 815us/step - loss: 6.2625e-09 - mse: 6.2625e-09 - val_loss: 7.6221e-09 - val_mse: 7.6221e-09\n",
      "Epoch 226/600\n",
      "949/949 [==============================] - 1s 815us/step - loss: 6.2818e-09 - mse: 6.2818e-09 - val_loss: 5.7992e-09 - val_mse: 5.7992e-09\n",
      "Epoch 227/600\n",
      "949/949 [==============================] - 1s 819us/step - loss: 6.1218e-09 - mse: 6.1218e-09 - val_loss: 6.2588e-09 - val_mse: 6.2588e-09\n",
      "Epoch 228/600\n",
      "949/949 [==============================] - 1s 810us/step - loss: 6.1975e-09 - mse: 6.1975e-09 - val_loss: 5.6797e-09 - val_mse: 5.6797e-09\n",
      "Epoch 229/600\n",
      "949/949 [==============================] - 1s 819us/step - loss: 6.1966e-09 - mse: 6.1966e-09 - val_loss: 7.4628e-09 - val_mse: 7.4628e-09\n",
      "Epoch 230/600\n",
      "949/949 [==============================] - 1s 817us/step - loss: 6.1444e-09 - mse: 6.1444e-09 - val_loss: 6.2376e-09 - val_mse: 6.2376e-09\n",
      "Epoch 231/600\n",
      "949/949 [==============================] - 1s 832us/step - loss: 6.2529e-09 - mse: 6.2529e-09 - val_loss: 5.6902e-09 - val_mse: 5.6902e-09\n",
      "Epoch 232/600\n",
      "949/949 [==============================] - 1s 823us/step - loss: 5.7153e-09 - mse: 5.7153e-09 - val_loss: 5.9673e-09 - val_mse: 5.9673e-09\n",
      "Epoch 233/600\n",
      "949/949 [==============================] - 1s 810us/step - loss: 5.5716e-09 - mse: 5.5716e-09 - val_loss: 6.0512e-09 - val_mse: 6.0512e-09\n",
      "Epoch 234/600\n",
      "949/949 [==============================] - 1s 827us/step - loss: 5.6408e-09 - mse: 5.6408e-09 - val_loss: 5.5791e-09 - val_mse: 5.5791e-09\n",
      "Epoch 235/600\n",
      "949/949 [==============================] - 1s 810us/step - loss: 5.6357e-09 - mse: 5.6357e-09 - val_loss: 7.2468e-09 - val_mse: 7.2468e-09\n",
      "Epoch 236/600\n",
      "949/949 [==============================] - 1s 815us/step - loss: 5.6509e-09 - mse: 5.6509e-09 - val_loss: 5.2884e-09 - val_mse: 5.2884e-09\n",
      "Epoch 237/600\n",
      "949/949 [==============================] - 1s 815us/step - loss: 5.5618e-09 - mse: 5.5618e-09 - val_loss: 6.1004e-09 - val_mse: 6.1004e-09\n",
      "Epoch 238/600\n",
      "949/949 [==============================] - 1s 815us/step - loss: 5.5773e-09 - mse: 5.5773e-09 - val_loss: 5.3449e-09 - val_mse: 5.3449e-09\n",
      "Epoch 239/600\n",
      "949/949 [==============================] - 1s 819us/step - loss: 5.6036e-09 - mse: 5.6036e-09 - val_loss: 6.4082e-09 - val_mse: 6.4082e-09\n",
      "Epoch 240/600\n",
      "949/949 [==============================] - 1s 815us/step - loss: 5.6222e-09 - mse: 5.6222e-09 - val_loss: 6.1340e-09 - val_mse: 6.1340e-09\n",
      "Epoch 241/600\n",
      "949/949 [==============================] - 1s 819us/step - loss: 5.6167e-09 - mse: 5.6167e-09 - val_loss: 5.7898e-09 - val_mse: 5.7898e-09\n",
      "Epoch 242/600\n",
      "949/949 [==============================] - 1s 819us/step - loss: 5.6329e-09 - mse: 5.6329e-09 - val_loss: 6.1179e-09 - val_mse: 6.1179e-09\n",
      "Epoch 243/600\n",
      "949/949 [==============================] - 1s 819us/step - loss: 5.6670e-09 - mse: 5.6670e-09 - val_loss: 5.4020e-09 - val_mse: 5.4020e-09\n",
      "Epoch 244/600\n",
      "949/949 [==============================] - 1s 810us/step - loss: 5.5749e-09 - mse: 5.5749e-09 - val_loss: 5.5304e-09 - val_mse: 5.5304e-09\n",
      "Epoch 245/600\n",
      "949/949 [==============================] - 1s 815us/step - loss: 5.5585e-09 - mse: 5.5585e-09 - val_loss: 5.4026e-09 - val_mse: 5.4026e-09\n",
      "Epoch 246/600\n",
      "949/949 [==============================] - 1s 803us/step - loss: 5.5216e-09 - mse: 5.5216e-09 - val_loss: 5.4449e-09 - val_mse: 5.4449e-09\n",
      "Epoch 247/600\n",
      "949/949 [==============================] - 1s 819us/step - loss: 5.5260e-09 - mse: 5.5260e-09 - val_loss: 5.5518e-09 - val_mse: 5.5518e-09\n",
      "Epoch 248/600\n",
      "949/949 [==============================] - 1s 815us/step - loss: 5.5658e-09 - mse: 5.5658e-09 - val_loss: 5.9057e-09 - val_mse: 5.9057e-09\n",
      "Epoch 249/600\n",
      "949/949 [==============================] - 1s 823us/step - loss: 5.5006e-09 - mse: 5.5006e-09 - val_loss: 5.6750e-09 - val_mse: 5.6750e-09\n",
      "Epoch 250/600\n",
      "949/949 [==============================] - 1s 819us/step - loss: 5.5079e-09 - mse: 5.5079e-09 - val_loss: 5.4485e-09 - val_mse: 5.4485e-09\n",
      "Epoch 251/600\n",
      "949/949 [==============================] - 1s 802us/step - loss: 5.5606e-09 - mse: 5.5606e-09 - val_loss: 5.5901e-09 - val_mse: 5.5901e-09\n",
      "Epoch 252/600\n",
      "949/949 [==============================] - 1s 815us/step - loss: 5.5932e-09 - mse: 5.5932e-09 - val_loss: 6.0928e-09 - val_mse: 6.0928e-09\n",
      "Epoch 253/600\n",
      "949/949 [==============================] - 1s 810us/step - loss: 5.5010e-09 - mse: 5.5010e-09 - val_loss: 5.4567e-09 - val_mse: 5.4567e-09\n",
      "Epoch 254/600\n",
      "949/949 [==============================] - 1s 815us/step - loss: 5.5471e-09 - mse: 5.5471e-09 - val_loss: 5.5253e-09 - val_mse: 5.5253e-09\n",
      "Epoch 255/600\n",
      "949/949 [==============================] - 1s 831us/step - loss: 5.5081e-09 - mse: 5.5081e-09 - val_loss: 6.2635e-09 - val_mse: 6.2635e-09\n",
      "Epoch 256/600\n",
      "949/949 [==============================] - 1s 830us/step - loss: 5.5770e-09 - mse: 5.5770e-09 - val_loss: 5.8623e-09 - val_mse: 5.8623e-09\n",
      "Epoch 257/600\n",
      "949/949 [==============================] - 1s 819us/step - loss: 5.4470e-09 - mse: 5.4470e-09 - val_loss: 5.5393e-09 - val_mse: 5.5393e-09\n",
      "Epoch 258/600\n",
      "949/949 [==============================] - 1s 815us/step - loss: 5.1758e-09 - mse: 5.1758e-09 - val_loss: 5.2076e-09 - val_mse: 5.2076e-09\n",
      "Epoch 259/600\n",
      "949/949 [==============================] - 1s 815us/step - loss: 5.1566e-09 - mse: 5.1566e-09 - val_loss: 5.6446e-09 - val_mse: 5.6446e-09\n",
      "Epoch 260/600\n",
      "949/949 [==============================] - 1s 819us/step - loss: 5.1804e-09 - mse: 5.1804e-09 - val_loss: 5.1845e-09 - val_mse: 5.1845e-09\n",
      "Epoch 261/600\n",
      "949/949 [==============================] - 1s 817us/step - loss: 5.1966e-09 - mse: 5.1966e-09 - val_loss: 5.1348e-09 - val_mse: 5.1348e-09\n",
      "Epoch 262/600\n",
      "949/949 [==============================] - 1s 810us/step - loss: 5.1657e-09 - mse: 5.1657e-09 - val_loss: 5.3269e-09 - val_mse: 5.3269e-09\n",
      "Epoch 263/600\n",
      "949/949 [==============================] - 1s 819us/step - loss: 5.2315e-09 - mse: 5.2315e-09 - val_loss: 5.2765e-09 - val_mse: 5.2765e-09\n",
      "Epoch 264/600\n",
      "949/949 [==============================] - 1s 815us/step - loss: 5.2001e-09 - mse: 5.2001e-09 - val_loss: 5.5778e-09 - val_mse: 5.5778e-09\n",
      "Epoch 265/600\n",
      "949/949 [==============================] - 1s 819us/step - loss: 5.1663e-09 - mse: 5.1663e-09 - val_loss: 5.4879e-09 - val_mse: 5.4879e-09\n",
      "Epoch 266/600\n",
      "949/949 [==============================] - 1s 827us/step - loss: 5.2698e-09 - mse: 5.2698e-09 - val_loss: 5.2918e-09 - val_mse: 5.2918e-09\n",
      "Epoch 267/600\n",
      "949/949 [==============================] - 1s 819us/step - loss: 5.1489e-09 - mse: 5.1489e-09 - val_loss: 5.3270e-09 - val_mse: 5.3270e-09\n",
      "Epoch 268/600\n",
      "949/949 [==============================] - 1s 827us/step - loss: 5.1744e-09 - mse: 5.1744e-09 - val_loss: 5.5460e-09 - val_mse: 5.5460e-09\n",
      "Epoch 269/600\n",
      "949/949 [==============================] - 1s 831us/step - loss: 5.1839e-09 - mse: 5.1839e-09 - val_loss: 5.4078e-09 - val_mse: 5.4078e-09\n",
      "Epoch 270/600\n",
      "949/949 [==============================] - 1s 823us/step - loss: 5.1534e-09 - mse: 5.1534e-09 - val_loss: 5.1704e-09 - val_mse: 5.1704e-09\n",
      "Epoch 271/600\n",
      "949/949 [==============================] - 1s 823us/step - loss: 5.1943e-09 - mse: 5.1943e-09 - val_loss: 5.3462e-09 - val_mse: 5.3462e-09\n",
      "Epoch 272/600\n",
      "949/949 [==============================] - 1s 815us/step - loss: 5.1499e-09 - mse: 5.1499e-09 - val_loss: 5.2097e-09 - val_mse: 5.2097e-09\n",
      "Epoch 273/600\n",
      "949/949 [==============================] - 1s 823us/step - loss: 5.1466e-09 - mse: 5.1466e-09 - val_loss: 5.2164e-09 - val_mse: 5.2164e-09\n",
      "Epoch 274/600\n",
      "949/949 [==============================] - 1s 832us/step - loss: 5.1400e-09 - mse: 5.1400e-09 - val_loss: 5.3221e-09 - val_mse: 5.3221e-09\n",
      "Epoch 275/600\n",
      "949/949 [==============================] - 1s 823us/step - loss: 5.1745e-09 - mse: 5.1745e-09 - val_loss: 6.1875e-09 - val_mse: 6.1875e-09\n",
      "Epoch 276/600\n",
      "949/949 [==============================] - 1s 824us/step - loss: 5.1329e-09 - mse: 5.1329e-09 - val_loss: 5.3175e-09 - val_mse: 5.3175e-09\n",
      "Epoch 277/600\n",
      "949/949 [==============================] - 1s 815us/step - loss: 5.1702e-09 - mse: 5.1702e-09 - val_loss: 5.1985e-09 - val_mse: 5.1985e-09\n",
      "Epoch 278/600\n",
      "949/949 [==============================] - 1s 819us/step - loss: 5.1640e-09 - mse: 5.1640e-09 - val_loss: 5.5891e-09 - val_mse: 5.5891e-09\n",
      "Epoch 279/600\n",
      "949/949 [==============================] - 1s 823us/step - loss: 5.1558e-09 - mse: 5.1558e-09 - val_loss: 5.1824e-09 - val_mse: 5.1824e-09\n",
      "Epoch 280/600\n",
      "949/949 [==============================] - 1s 823us/step - loss: 5.2028e-09 - mse: 5.2028e-09 - val_loss: 5.1968e-09 - val_mse: 5.1968e-09\n",
      "Epoch 281/600\n",
      "949/949 [==============================] - 1s 829us/step - loss: 5.1452e-09 - mse: 5.1452e-09 - val_loss: 5.4103e-09 - val_mse: 5.4103e-09\n",
      "Epoch 282/600\n",
      "949/949 [==============================] - 1s 840us/step - loss: 5.1314e-09 - mse: 5.1314e-09 - val_loss: 5.0677e-09 - val_mse: 5.0677e-09\n",
      "Epoch 283/600\n",
      "949/949 [==============================] - 1s 823us/step - loss: 5.0513e-09 - mse: 5.0513e-09 - val_loss: 5.0735e-09 - val_mse: 5.0735e-09\n",
      "Epoch 284/600\n",
      "949/949 [==============================] - 1s 815us/step - loss: 4.9306e-09 - mse: 4.9306e-09 - val_loss: 5.0486e-09 - val_mse: 5.0486e-09\n",
      "Epoch 285/600\n",
      "949/949 [==============================] - 1s 820us/step - loss: 4.9125e-09 - mse: 4.9125e-09 - val_loss: 5.0445e-09 - val_mse: 5.0445e-09\n",
      "Epoch 286/600\n",
      "949/949 [==============================] - 1s 819us/step - loss: 4.9380e-09 - mse: 4.9380e-09 - val_loss: 5.0802e-09 - val_mse: 5.0802e-09\n",
      "Epoch 287/600\n",
      "949/949 [==============================] - 1s 826us/step - loss: 4.9399e-09 - mse: 4.9399e-09 - val_loss: 5.0164e-09 - val_mse: 5.0164e-09\n",
      "Epoch 288/600\n",
      "949/949 [==============================] - 1s 814us/step - loss: 4.9206e-09 - mse: 4.9206e-09 - val_loss: 5.0163e-09 - val_mse: 5.0163e-09\n",
      "Epoch 289/600\n",
      "949/949 [==============================] - 1s 815us/step - loss: 4.9168e-09 - mse: 4.9168e-09 - val_loss: 5.1177e-09 - val_mse: 5.1177e-09\n",
      "Epoch 290/600\n",
      "949/949 [==============================] - 1s 830us/step - loss: 4.9107e-09 - mse: 4.9107e-09 - val_loss: 5.3439e-09 - val_mse: 5.3439e-09\n",
      "Epoch 291/600\n",
      "949/949 [==============================] - 1s 815us/step - loss: 4.9500e-09 - mse: 4.9500e-09 - val_loss: 5.3261e-09 - val_mse: 5.3261e-09\n",
      "Epoch 292/600\n",
      "949/949 [==============================] - 1s 819us/step - loss: 4.9556e-09 - mse: 4.9556e-09 - val_loss: 5.3565e-09 - val_mse: 5.3565e-09\n",
      "Epoch 293/600\n",
      "949/949 [==============================] - 1s 815us/step - loss: 4.9075e-09 - mse: 4.9075e-09 - val_loss: 4.9594e-09 - val_mse: 4.9594e-09\n",
      "Epoch 294/600\n",
      "949/949 [==============================] - 1s 819us/step - loss: 4.8978e-09 - mse: 4.8978e-09 - val_loss: 5.0168e-09 - val_mse: 5.0168e-09\n",
      "Epoch 295/600\n",
      "949/949 [==============================] - 1s 819us/step - loss: 4.9457e-09 - mse: 4.9457e-09 - val_loss: 4.9615e-09 - val_mse: 4.9615e-09\n",
      "Epoch 296/600\n",
      "949/949 [==============================] - 1s 810us/step - loss: 4.9171e-09 - mse: 4.9171e-09 - val_loss: 4.9114e-09 - val_mse: 4.9114e-09\n",
      "Epoch 297/600\n",
      "949/949 [==============================] - 1s 823us/step - loss: 4.8938e-09 - mse: 4.8938e-09 - val_loss: 5.3243e-09 - val_mse: 5.3243e-09\n",
      "Epoch 298/600\n",
      "949/949 [==============================] - 1s 823us/step - loss: 4.9327e-09 - mse: 4.9327e-09 - val_loss: 5.6623e-09 - val_mse: 5.6623e-09\n",
      "Epoch 299/600\n",
      "949/949 [==============================] - 1s 810us/step - loss: 4.9425e-09 - mse: 4.9425e-09 - val_loss: 4.8850e-09 - val_mse: 4.8850e-09\n",
      "Epoch 300/600\n",
      "949/949 [==============================] - 1s 817us/step - loss: 4.9123e-09 - mse: 4.9123e-09 - val_loss: 5.0056e-09 - val_mse: 5.0056e-09\n",
      "Epoch 301/600\n",
      "949/949 [==============================] - 1s 819us/step - loss: 4.9274e-09 - mse: 4.9274e-09 - val_loss: 5.2943e-09 - val_mse: 5.2943e-09\n",
      "Epoch 302/600\n",
      "949/949 [==============================] - 1s 810us/step - loss: 4.9160e-09 - mse: 4.9160e-09 - val_loss: 5.2513e-09 - val_mse: 5.2513e-09\n",
      "Epoch 303/600\n",
      "949/949 [==============================] - 1s 840us/step - loss: 4.8970e-09 - mse: 4.8970e-09 - val_loss: 4.9763e-09 - val_mse: 4.9763e-09\n",
      "Epoch 304/600\n",
      "949/949 [==============================] - 1s 823us/step - loss: 4.9332e-09 - mse: 4.9332e-09 - val_loss: 5.0581e-09 - val_mse: 5.0581e-09\n",
      "Epoch 305/600\n",
      "949/949 [==============================] - 1s 815us/step - loss: 4.9162e-09 - mse: 4.9162e-09 - val_loss: 5.2401e-09 - val_mse: 5.2401e-09\n",
      "Epoch 306/600\n",
      "949/949 [==============================] - 1s 819us/step - loss: 4.9169e-09 - mse: 4.9169e-09 - val_loss: 5.1184e-09 - val_mse: 5.1184e-09\n",
      "Epoch 307/600\n",
      "949/949 [==============================] - 1s 815us/step - loss: 4.9040e-09 - mse: 4.9040e-09 - val_loss: 4.9882e-09 - val_mse: 4.9882e-09\n",
      "Epoch 308/600\n",
      "949/949 [==============================] - 1s 827us/step - loss: 4.8965e-09 - mse: 4.8965e-09 - val_loss: 4.8998e-09 - val_mse: 4.8998e-09\n",
      "Epoch 309/600\n",
      "949/949 [==============================] - 1s 817us/step - loss: 4.8001e-09 - mse: 4.8001e-09 - val_loss: 4.9973e-09 - val_mse: 4.9973e-09\n",
      "Epoch 310/600\n",
      "949/949 [==============================] - 1s 840us/step - loss: 4.7696e-09 - mse: 4.7696e-09 - val_loss: 4.9239e-09 - val_mse: 4.9239e-09\n",
      "Epoch 311/600\n",
      "949/949 [==============================] - 1s 815us/step - loss: 4.7805e-09 - mse: 4.7805e-09 - val_loss: 4.9570e-09 - val_mse: 4.9570e-09\n",
      "Epoch 312/600\n",
      "949/949 [==============================] - 1s 815us/step - loss: 4.7649e-09 - mse: 4.7649e-09 - val_loss: 4.9988e-09 - val_mse: 4.9988e-09\n",
      "Epoch 313/600\n",
      "949/949 [==============================] - 1s 817us/step - loss: 4.7700e-09 - mse: 4.7700e-09 - val_loss: 4.8580e-09 - val_mse: 4.8580e-09\n",
      "Epoch 314/600\n",
      "949/949 [==============================] - 1s 832us/step - loss: 4.7781e-09 - mse: 4.7781e-09 - val_loss: 4.8739e-09 - val_mse: 4.8739e-09\n",
      "Epoch 315/600\n",
      "949/949 [==============================] - 1s 819us/step - loss: 4.7768e-09 - mse: 4.7768e-09 - val_loss: 4.8408e-09 - val_mse: 4.8408e-09\n",
      "Epoch 316/600\n",
      "949/949 [==============================] - 1s 823us/step - loss: 4.7680e-09 - mse: 4.7680e-09 - val_loss: 5.1158e-09 - val_mse: 5.1158e-09\n",
      "Epoch 317/600\n",
      "949/949 [==============================] - 1s 811us/step - loss: 4.7760e-09 - mse: 4.7760e-09 - val_loss: 5.0240e-09 - val_mse: 5.0240e-09\n",
      "Epoch 318/600\n",
      "949/949 [==============================] - 1s 827us/step - loss: 4.7941e-09 - mse: 4.7941e-09 - val_loss: 5.0700e-09 - val_mse: 5.0700e-09\n",
      "Epoch 319/600\n",
      "949/949 [==============================] - 1s 806us/step - loss: 4.7674e-09 - mse: 4.7674e-09 - val_loss: 4.9294e-09 - val_mse: 4.9294e-09\n",
      "Epoch 320/600\n",
      "949/949 [==============================] - 1s 815us/step - loss: 4.7679e-09 - mse: 4.7679e-09 - val_loss: 4.8538e-09 - val_mse: 4.8538e-09\n",
      "Epoch 321/600\n",
      "949/949 [==============================] - 1s 806us/step - loss: 4.7916e-09 - mse: 4.7916e-09 - val_loss: 5.1565e-09 - val_mse: 5.1565e-09\n",
      "Epoch 322/600\n",
      "949/949 [==============================] - 1s 823us/step - loss: 4.7758e-09 - mse: 4.7758e-09 - val_loss: 5.0686e-09 - val_mse: 5.0686e-09\n",
      "Epoch 323/600\n",
      "949/949 [==============================] - 1s 819us/step - loss: 4.7755e-09 - mse: 4.7755e-09 - val_loss: 4.8922e-09 - val_mse: 4.8922e-09\n",
      "Epoch 324/600\n",
      "949/949 [==============================] - 1s 823us/step - loss: 4.7790e-09 - mse: 4.7790e-09 - val_loss: 5.0106e-09 - val_mse: 5.0106e-09\n",
      "Epoch 325/600\n",
      "949/949 [==============================] - 1s 831us/step - loss: 4.7474e-09 - mse: 4.7474e-09 - val_loss: 4.8383e-09 - val_mse: 4.8383e-09\n",
      "Epoch 326/600\n",
      "949/949 [==============================] - 1s 815us/step - loss: 4.7642e-09 - mse: 4.7642e-09 - val_loss: 5.0742e-09 - val_mse: 5.0742e-09\n",
      "Epoch 327/600\n",
      "949/949 [==============================] - 1s 825us/step - loss: 4.7916e-09 - mse: 4.7916e-09 - val_loss: 4.8405e-09 - val_mse: 4.8405e-09\n",
      "Epoch 328/600\n",
      "949/949 [==============================] - 1s 815us/step - loss: 4.7776e-09 - mse: 4.7776e-09 - val_loss: 4.8717e-09 - val_mse: 4.8717e-09\n",
      "Epoch 329/600\n",
      "949/949 [==============================] - 1s 815us/step - loss: 4.7835e-09 - mse: 4.7835e-09 - val_loss: 4.8579e-09 - val_mse: 4.8579e-09\n",
      "Epoch 330/600\n",
      "949/949 [==============================] - 1s 819us/step - loss: 4.7669e-09 - mse: 4.7669e-09 - val_loss: 4.9909e-09 - val_mse: 4.9909e-09\n",
      "Epoch 331/600\n",
      "949/949 [==============================] - 1s 827us/step - loss: 4.7604e-09 - mse: 4.7604e-09 - val_loss: 4.8433e-09 - val_mse: 4.8433e-09\n",
      "Epoch 332/600\n",
      "949/949 [==============================] - 1s 823us/step - loss: 4.7443e-09 - mse: 4.7443e-09 - val_loss: 4.8661e-09 - val_mse: 4.8661e-09\n",
      "Epoch 333/600\n",
      "949/949 [==============================] - 1s 819us/step - loss: 4.7754e-09 - mse: 4.7754e-09 - val_loss: 5.0038e-09 - val_mse: 5.0038e-09\n",
      "Epoch 334/600\n",
      "949/949 [==============================] - 1s 815us/step - loss: 4.7560e-09 - mse: 4.7560e-09 - val_loss: 4.8324e-09 - val_mse: 4.8324e-09\n",
      "Epoch 335/600\n",
      "949/949 [==============================] - 1s 823us/step - loss: 4.6823e-09 - mse: 4.6823e-09 - val_loss: 4.8400e-09 - val_mse: 4.8400e-09\n",
      "Epoch 336/600\n",
      "949/949 [==============================] - 1s 821us/step - loss: 4.6879e-09 - mse: 4.6879e-09 - val_loss: 4.8229e-09 - val_mse: 4.8229e-09\n",
      "Epoch 337/600\n",
      "949/949 [==============================] - 1s 823us/step - loss: 4.6844e-09 - mse: 4.6844e-09 - val_loss: 4.8940e-09 - val_mse: 4.8940e-09\n",
      "Epoch 338/600\n",
      "949/949 [==============================] - 1s 827us/step - loss: 4.6935e-09 - mse: 4.6935e-09 - val_loss: 4.8143e-09 - val_mse: 4.8143e-09\n",
      "Epoch 339/600\n",
      "949/949 [==============================] - 1s 823us/step - loss: 4.6842e-09 - mse: 4.6842e-09 - val_loss: 5.0552e-09 - val_mse: 5.0552e-09\n",
      "Epoch 340/600\n",
      "949/949 [==============================] - 1s 840us/step - loss: 4.6777e-09 - mse: 4.6777e-09 - val_loss: 4.8038e-09 - val_mse: 4.8038e-09\n",
      "Epoch 341/600\n",
      "949/949 [==============================] - 1s 827us/step - loss: 4.6816e-09 - mse: 4.6816e-09 - val_loss: 4.8346e-09 - val_mse: 4.8346e-09\n",
      "Epoch 342/600\n",
      "949/949 [==============================] - 1s 810us/step - loss: 4.6775e-09 - mse: 4.6775e-09 - val_loss: 4.7987e-09 - val_mse: 4.7987e-09\n",
      "Epoch 343/600\n",
      "949/949 [==============================] - 1s 823us/step - loss: 4.6807e-09 - mse: 4.6807e-09 - val_loss: 4.9065e-09 - val_mse: 4.9065e-09\n",
      "Epoch 344/600\n",
      "949/949 [==============================] - 1s 823us/step - loss: 4.6816e-09 - mse: 4.6816e-09 - val_loss: 4.9665e-09 - val_mse: 4.9665e-09\n",
      "Epoch 345/600\n",
      "949/949 [==============================] - 1s 823us/step - loss: 4.6919e-09 - mse: 4.6919e-09 - val_loss: 4.9704e-09 - val_mse: 4.9704e-09\n",
      "Epoch 346/600\n",
      "949/949 [==============================] - 1s 819us/step - loss: 4.6828e-09 - mse: 4.6828e-09 - val_loss: 4.9562e-09 - val_mse: 4.9562e-09\n",
      "Epoch 347/600\n",
      "949/949 [==============================] - 1s 823us/step - loss: 4.6711e-09 - mse: 4.6711e-09 - val_loss: 4.8283e-09 - val_mse: 4.8283e-09\n",
      "Epoch 348/600\n",
      "949/949 [==============================] - 1s 827us/step - loss: 4.6838e-09 - mse: 4.6838e-09 - val_loss: 4.8786e-09 - val_mse: 4.8786e-09\n",
      "Epoch 349/600\n",
      "949/949 [==============================] - 1s 815us/step - loss: 4.6810e-09 - mse: 4.6810e-09 - val_loss: 4.8069e-09 - val_mse: 4.8069e-09\n",
      "Epoch 350/600\n",
      "949/949 [==============================] - 1s 823us/step - loss: 4.6697e-09 - mse: 4.6697e-09 - val_loss: 4.9223e-09 - val_mse: 4.9223e-09\n",
      "Epoch 351/600\n",
      "949/949 [==============================] - 1s 836us/step - loss: 4.6870e-09 - mse: 4.6870e-09 - val_loss: 4.8707e-09 - val_mse: 4.8707e-09\n",
      "Epoch 352/600\n",
      "949/949 [==============================] - 1s 823us/step - loss: 4.6819e-09 - mse: 4.6819e-09 - val_loss: 4.9356e-09 - val_mse: 4.9356e-09\n",
      "Epoch 353/600\n",
      "949/949 [==============================] - 1s 815us/step - loss: 4.6837e-09 - mse: 4.6837e-09 - val_loss: 4.8464e-09 - val_mse: 4.8464e-09\n",
      "Epoch 354/600\n",
      "949/949 [==============================] - 1s 815us/step - loss: 4.6798e-09 - mse: 4.6798e-09 - val_loss: 4.8875e-09 - val_mse: 4.8875e-09\n",
      "Epoch 355/600\n",
      "949/949 [==============================] - 1s 823us/step - loss: 4.6692e-09 - mse: 4.6692e-09 - val_loss: 4.8938e-09 - val_mse: 4.8938e-09\n",
      "Epoch 356/600\n",
      "949/949 [==============================] - 1s 815us/step - loss: 4.6820e-09 - mse: 4.6820e-09 - val_loss: 4.9210e-09 - val_mse: 4.9210e-09\n",
      "Epoch 357/600\n",
      "949/949 [==============================] - 1s 815us/step - loss: 4.6743e-09 - mse: 4.6743e-09 - val_loss: 4.8156e-09 - val_mse: 4.8156e-09\n",
      "Epoch 358/600\n",
      "949/949 [==============================] - 1s 819us/step - loss: 4.6727e-09 - mse: 4.6727e-09 - val_loss: 4.8194e-09 - val_mse: 4.8194e-09\n",
      "Epoch 359/600\n",
      "949/949 [==============================] - 1s 815us/step - loss: 4.6677e-09 - mse: 4.6677e-09 - val_loss: 4.9125e-09 - val_mse: 4.9125e-09\n",
      "Epoch 360/600\n",
      "949/949 [==============================] - 1s 823us/step - loss: 4.6522e-09 - mse: 4.6522e-09 - val_loss: 4.8459e-09 - val_mse: 4.8459e-09\n",
      "Epoch 361/600\n",
      "949/949 [==============================] - 1s 819us/step - loss: 4.6239e-09 - mse: 4.6239e-09 - val_loss: 4.7827e-09 - val_mse: 4.7827e-09\n",
      "Epoch 362/600\n",
      "949/949 [==============================] - 1s 823us/step - loss: 4.6252e-09 - mse: 4.6252e-09 - val_loss: 4.7796e-09 - val_mse: 4.7796e-09\n",
      "Epoch 363/600\n",
      "949/949 [==============================] - 1s 815us/step - loss: 4.6291e-09 - mse: 4.6291e-09 - val_loss: 4.8433e-09 - val_mse: 4.8433e-09\n",
      "Epoch 364/600\n",
      "949/949 [==============================] - 1s 808us/step - loss: 4.6207e-09 - mse: 4.6207e-09 - val_loss: 4.7784e-09 - val_mse: 4.7784e-09\n",
      "Epoch 365/600\n",
      "949/949 [==============================] - 1s 810us/step - loss: 4.6302e-09 - mse: 4.6302e-09 - val_loss: 4.7868e-09 - val_mse: 4.7868e-09\n",
      "Epoch 366/600\n",
      "949/949 [==============================] - 1s 810us/step - loss: 4.6200e-09 - mse: 4.6200e-09 - val_loss: 4.8480e-09 - val_mse: 4.8480e-09\n",
      "Epoch 367/600\n",
      "949/949 [==============================] - 1s 810us/step - loss: 4.6285e-09 - mse: 4.6285e-09 - val_loss: 4.8019e-09 - val_mse: 4.8019e-09\n",
      "Epoch 368/600\n",
      "949/949 [==============================] - 1s 810us/step - loss: 4.6198e-09 - mse: 4.6198e-09 - val_loss: 4.8089e-09 - val_mse: 4.8089e-09\n",
      "Epoch 369/600\n",
      "949/949 [==============================] - 1s 810us/step - loss: 4.6279e-09 - mse: 4.6279e-09 - val_loss: 4.7975e-09 - val_mse: 4.7975e-09\n",
      "Epoch 370/600\n",
      "949/949 [==============================] - 1s 806us/step - loss: 4.6267e-09 - mse: 4.6267e-09 - val_loss: 4.7619e-09 - val_mse: 4.7619e-09\n",
      "Epoch 371/600\n",
      "949/949 [==============================] - 1s 815us/step - loss: 4.6253e-09 - mse: 4.6253e-09 - val_loss: 4.7619e-09 - val_mse: 4.7619e-09\n",
      "Epoch 372/600\n",
      "949/949 [==============================] - 1s 819us/step - loss: 4.6240e-09 - mse: 4.6240e-09 - val_loss: 4.7948e-09 - val_mse: 4.7948e-09\n",
      "Epoch 373/600\n",
      "949/949 [==============================] - 1s 802us/step - loss: 4.6216e-09 - mse: 4.6216e-09 - val_loss: 4.7726e-09 - val_mse: 4.7726e-09\n",
      "Epoch 374/600\n",
      "949/949 [==============================] - 1s 806us/step - loss: 4.6202e-09 - mse: 4.6202e-09 - val_loss: 4.7986e-09 - val_mse: 4.7986e-09\n",
      "Epoch 375/600\n",
      "949/949 [==============================] - 1s 810us/step - loss: 4.6269e-09 - mse: 4.6269e-09 - val_loss: 4.8028e-09 - val_mse: 4.8028e-09\n",
      "Epoch 376/600\n",
      "949/949 [==============================] - 1s 819us/step - loss: 4.6250e-09 - mse: 4.6250e-09 - val_loss: 4.8301e-09 - val_mse: 4.8301e-09\n",
      "Epoch 377/600\n",
      "949/949 [==============================] - 1s 813us/step - loss: 4.6235e-09 - mse: 4.6235e-09 - val_loss: 4.8465e-09 - val_mse: 4.8465e-09\n",
      "Epoch 378/600\n",
      "949/949 [==============================] - 1s 815us/step - loss: 4.6238e-09 - mse: 4.6238e-09 - val_loss: 4.8515e-09 - val_mse: 4.8515e-09\n",
      "Epoch 379/600\n",
      "949/949 [==============================] - 1s 815us/step - loss: 4.6214e-09 - mse: 4.6214e-09 - val_loss: 4.8199e-09 - val_mse: 4.8199e-09\n",
      "Epoch 380/600\n",
      "949/949 [==============================] - 1s 831us/step - loss: 4.6224e-09 - mse: 4.6224e-09 - val_loss: 4.7756e-09 - val_mse: 4.7756e-09\n",
      "Epoch 381/600\n",
      "949/949 [==============================] - 1s 836us/step - loss: 4.6204e-09 - mse: 4.6204e-09 - val_loss: 4.9477e-09 - val_mse: 4.9477e-09\n",
      "Epoch 382/600\n",
      "949/949 [==============================] - 1s 827us/step - loss: 4.6204e-09 - mse: 4.6204e-09 - val_loss: 4.7757e-09 - val_mse: 4.7757e-09\n",
      "Epoch 383/600\n",
      "949/949 [==============================] - 1s 823us/step - loss: 4.6208e-09 - mse: 4.6208e-09 - val_loss: 4.8864e-09 - val_mse: 4.8864e-09\n",
      "Epoch 384/600\n",
      "949/949 [==============================] - 1s 819us/step - loss: 4.6224e-09 - mse: 4.6224e-09 - val_loss: 4.8087e-09 - val_mse: 4.8087e-09\n",
      "Epoch 385/600\n",
      "949/949 [==============================] - 1s 823us/step - loss: 4.6245e-09 - mse: 4.6245e-09 - val_loss: 4.8040e-09 - val_mse: 4.8040e-09\n",
      "Epoch 386/600\n",
      "949/949 [==============================] - 1s 811us/step - loss: 4.5998e-09 - mse: 4.5998e-09 - val_loss: 4.7602e-09 - val_mse: 4.7602e-09\n",
      "Epoch 387/600\n",
      "949/949 [==============================] - 1s 806us/step - loss: 4.5866e-09 - mse: 4.5866e-09 - val_loss: 4.7645e-09 - val_mse: 4.7645e-09\n",
      "Epoch 388/600\n",
      "949/949 [==============================] - 1s 819us/step - loss: 4.5914e-09 - mse: 4.5914e-09 - val_loss: 4.7533e-09 - val_mse: 4.7533e-09\n",
      "Epoch 389/600\n",
      "949/949 [==============================] - 1s 810us/step - loss: 4.5918e-09 - mse: 4.5918e-09 - val_loss: 4.8385e-09 - val_mse: 4.8385e-09\n",
      "Epoch 390/600\n",
      "949/949 [==============================] - 1s 819us/step - loss: 4.5913e-09 - mse: 4.5913e-09 - val_loss: 4.7937e-09 - val_mse: 4.7937e-09\n",
      "Epoch 391/600\n",
      "949/949 [==============================] - 1s 819us/step - loss: 4.5913e-09 - mse: 4.5913e-09 - val_loss: 4.7588e-09 - val_mse: 4.7588e-09\n",
      "Epoch 392/600\n",
      "949/949 [==============================] - 1s 798us/step - loss: 4.5919e-09 - mse: 4.5919e-09 - val_loss: 4.7807e-09 - val_mse: 4.7807e-09\n",
      "Epoch 393/600\n",
      "949/949 [==============================] - 1s 802us/step - loss: 4.5921e-09 - mse: 4.5921e-09 - val_loss: 4.7579e-09 - val_mse: 4.7579e-09\n",
      "Epoch 394/600\n",
      "949/949 [==============================] - 1s 798us/step - loss: 4.5933e-09 - mse: 4.5933e-09 - val_loss: 4.8154e-09 - val_mse: 4.8154e-09\n",
      "Epoch 395/600\n",
      "949/949 [==============================] - 1s 806us/step - loss: 4.5945e-09 - mse: 4.5945e-09 - val_loss: 4.7978e-09 - val_mse: 4.7978e-09\n",
      "Epoch 396/600\n",
      "949/949 [==============================] - 1s 815us/step - loss: 4.5904e-09 - mse: 4.5904e-09 - val_loss: 4.7575e-09 - val_mse: 4.7575e-09\n",
      "Epoch 397/600\n",
      "949/949 [==============================] - 1s 810us/step - loss: 4.5899e-09 - mse: 4.5899e-09 - val_loss: 4.7967e-09 - val_mse: 4.7967e-09\n",
      "Epoch 398/600\n",
      "949/949 [==============================] - 1s 806us/step - loss: 4.5920e-09 - mse: 4.5920e-09 - val_loss: 4.8050e-09 - val_mse: 4.8050e-09\n",
      "Epoch 399/600\n",
      "949/949 [==============================] - 1s 806us/step - loss: 4.5890e-09 - mse: 4.5890e-09 - val_loss: 4.7609e-09 - val_mse: 4.7609e-09\n",
      "Epoch 400/600\n",
      "949/949 [==============================] - 1s 798us/step - loss: 4.5888e-09 - mse: 4.5888e-09 - val_loss: 4.7856e-09 - val_mse: 4.7856e-09\n",
      "Epoch 401/600\n",
      "949/949 [==============================] - 1s 816us/step - loss: 4.5865e-09 - mse: 4.5865e-09 - val_loss: 4.7850e-09 - val_mse: 4.7850e-09\n",
      "Epoch 402/600\n",
      "949/949 [==============================] - 1s 810us/step - loss: 4.5912e-09 - mse: 4.5912e-09 - val_loss: 4.8084e-09 - val_mse: 4.8084e-09\n",
      "Epoch 403/600\n",
      "949/949 [==============================] - 1s 810us/step - loss: 4.5876e-09 - mse: 4.5876e-09 - val_loss: 4.7909e-09 - val_mse: 4.7909e-09\n",
      "Epoch 404/600\n",
      "949/949 [==============================] - 1s 810us/step - loss: 4.5931e-09 - mse: 4.5931e-09 - val_loss: 4.7720e-09 - val_mse: 4.7720e-09\n",
      "Epoch 405/600\n",
      "949/949 [==============================] - 1s 815us/step - loss: 4.5893e-09 - mse: 4.5893e-09 - val_loss: 4.7775e-09 - val_mse: 4.7775e-09\n",
      "Epoch 406/600\n",
      "949/949 [==============================] - 1s 810us/step - loss: 4.5906e-09 - mse: 4.5906e-09 - val_loss: 4.8038e-09 - val_mse: 4.8038e-09\n",
      "Epoch 407/600\n",
      "949/949 [==============================] - 1s 819us/step - loss: 4.5917e-09 - mse: 4.5917e-09 - val_loss: 4.7574e-09 - val_mse: 4.7574e-09\n",
      "Epoch 408/600\n",
      "949/949 [==============================] - 1s 819us/step - loss: 4.5934e-09 - mse: 4.5934e-09 - val_loss: 4.7825e-09 - val_mse: 4.7825e-09\n",
      "Epoch 409/600\n",
      "949/949 [==============================] - 1s 827us/step - loss: 4.5895e-09 - mse: 4.5895e-09 - val_loss: 4.7897e-09 - val_mse: 4.7897e-09\n",
      "Epoch 410/600\n",
      "949/949 [==============================] - 1s 827us/step - loss: 4.5866e-09 - mse: 4.5866e-09 - val_loss: 4.7785e-09 - val_mse: 4.7785e-09\n",
      "Epoch 411/600\n",
      "949/949 [==============================] - 1s 823us/step - loss: 4.5907e-09 - mse: 4.5907e-09 - val_loss: 4.7586e-09 - val_mse: 4.7586e-09\n",
      "Epoch 412/600\n",
      "949/949 [==============================] - 1s 822us/step - loss: 4.5679e-09 - mse: 4.5679e-09 - val_loss: 4.7660e-09 - val_mse: 4.7660e-09\n",
      "Epoch 413/600\n",
      "949/949 [==============================] - 1s 819us/step - loss: 4.5701e-09 - mse: 4.5701e-09 - val_loss: 4.7877e-09 - val_mse: 4.7877e-09\n",
      "Epoch 414/600\n",
      "949/949 [==============================] - 1s 819us/step - loss: 4.5718e-09 - mse: 4.5718e-09 - val_loss: 4.7878e-09 - val_mse: 4.7878e-09\n",
      "Epoch 415/600\n",
      "949/949 [==============================] - 1s 823us/step - loss: 4.5676e-09 - mse: 4.5676e-09 - val_loss: 4.7677e-09 - val_mse: 4.7677e-09\n",
      "Epoch 416/600\n",
      "949/949 [==============================] - 1s 823us/step - loss: 4.5692e-09 - mse: 4.5692e-09 - val_loss: 4.7557e-09 - val_mse: 4.7557e-09\n",
      "Epoch 417/600\n",
      "949/949 [==============================] - 1s 823us/step - loss: 4.5689e-09 - mse: 4.5689e-09 - val_loss: 4.7543e-09 - val_mse: 4.7543e-09\n",
      "Epoch 418/600\n",
      "949/949 [==============================] - 1s 823us/step - loss: 4.5685e-09 - mse: 4.5685e-09 - val_loss: 4.7424e-09 - val_mse: 4.7424e-09\n",
      "Epoch 419/600\n",
      "949/949 [==============================] - 1s 819us/step - loss: 4.5671e-09 - mse: 4.5671e-09 - val_loss: 4.7730e-09 - val_mse: 4.7730e-09\n",
      "Epoch 420/600\n",
      "949/949 [==============================] - 1s 810us/step - loss: 4.5703e-09 - mse: 4.5703e-09 - val_loss: 4.7560e-09 - val_mse: 4.7560e-09\n",
      "Epoch 421/600\n",
      "949/949 [==============================] - 1s 810us/step - loss: 4.5687e-09 - mse: 4.5687e-09 - val_loss: 4.7679e-09 - val_mse: 4.7679e-09\n",
      "Epoch 422/600\n",
      "949/949 [==============================] - 1s 810us/step - loss: 4.5688e-09 - mse: 4.5688e-09 - val_loss: 4.7775e-09 - val_mse: 4.7775e-09\n",
      "Epoch 423/600\n",
      "949/949 [==============================] - 1s 831us/step - loss: 4.5704e-09 - mse: 4.5704e-09 - val_loss: 4.7543e-09 - val_mse: 4.7543e-09\n",
      "Epoch 424/600\n",
      "949/949 [==============================] - 1s 832us/step - loss: 4.5657e-09 - mse: 4.5657e-09 - val_loss: 4.7565e-09 - val_mse: 4.7565e-09\n",
      "Epoch 425/600\n",
      "949/949 [==============================] - 1s 827us/step - loss: 4.5684e-09 - mse: 4.5684e-09 - val_loss: 4.7677e-09 - val_mse: 4.7677e-09\n",
      "Epoch 426/600\n",
      "949/949 [==============================] - 1s 819us/step - loss: 4.5715e-09 - mse: 4.5715e-09 - val_loss: 4.7540e-09 - val_mse: 4.7540e-09\n",
      "Epoch 427/600\n",
      "949/949 [==============================] - 1s 820us/step - loss: 4.5695e-09 - mse: 4.5695e-09 - val_loss: 4.7626e-09 - val_mse: 4.7626e-09\n",
      "Epoch 428/600\n",
      "949/949 [==============================] - 1s 819us/step - loss: 4.5674e-09 - mse: 4.5674e-09 - val_loss: 4.7577e-09 - val_mse: 4.7577e-09\n",
      "Epoch 429/600\n",
      "949/949 [==============================] - 1s 814us/step - loss: 4.5693e-09 - mse: 4.5693e-09 - val_loss: 4.7501e-09 - val_mse: 4.7501e-09\n",
      "Epoch 430/600\n",
      "949/949 [==============================] - 1s 822us/step - loss: 4.5677e-09 - mse: 4.5677e-09 - val_loss: 4.7469e-09 - val_mse: 4.7469e-09\n",
      "Epoch 431/600\n",
      "949/949 [==============================] - 1s 823us/step - loss: 4.5695e-09 - mse: 4.5695e-09 - val_loss: 4.7659e-09 - val_mse: 4.7659e-09\n",
      "Epoch 432/600\n",
      "949/949 [==============================] - 1s 819us/step - loss: 4.5692e-09 - mse: 4.5692e-09 - val_loss: 4.7550e-09 - val_mse: 4.7550e-09\n",
      "Epoch 433/600\n",
      "949/949 [==============================] - 1s 810us/step - loss: 4.5678e-09 - mse: 4.5678e-09 - val_loss: 4.7397e-09 - val_mse: 4.7397e-09\n",
      "Epoch 434/600\n",
      "949/949 [==============================] - 1s 831us/step - loss: 4.5685e-09 - mse: 4.5685e-09 - val_loss: 4.7571e-09 - val_mse: 4.7571e-09\n",
      "Epoch 435/600\n",
      "949/949 [==============================] - 1s 815us/step - loss: 4.5661e-09 - mse: 4.5661e-09 - val_loss: 4.7529e-09 - val_mse: 4.7529e-09\n",
      "Epoch 436/600\n",
      "949/949 [==============================] - 1s 809us/step - loss: 4.5682e-09 - mse: 4.5682e-09 - val_loss: 4.7907e-09 - val_mse: 4.7907e-09\n",
      "Epoch 437/600\n",
      "949/949 [==============================] - 1s 815us/step - loss: 4.5642e-09 - mse: 4.5642e-09 - val_loss: 4.7386e-09 - val_mse: 4.7386e-09\n",
      "Epoch 438/600\n",
      "949/949 [==============================] - 1s 809us/step - loss: 4.5565e-09 - mse: 4.5565e-09 - val_loss: 4.7415e-09 - val_mse: 4.7415e-09\n",
      "Epoch 439/600\n",
      "949/949 [==============================] - 1s 815us/step - loss: 4.5563e-09 - mse: 4.5563e-09 - val_loss: 4.7557e-09 - val_mse: 4.7557e-09\n",
      "Epoch 440/600\n",
      "949/949 [==============================] - 1s 815us/step - loss: 4.5539e-09 - mse: 4.5539e-09 - val_loss: 4.7401e-09 - val_mse: 4.7401e-09\n",
      "Epoch 441/600\n",
      "949/949 [==============================] - 1s 810us/step - loss: 4.5573e-09 - mse: 4.5573e-09 - val_loss: 4.7455e-09 - val_mse: 4.7455e-09\n",
      "Epoch 442/600\n",
      "949/949 [==============================] - 1s 802us/step - loss: 4.5564e-09 - mse: 4.5564e-09 - val_loss: 4.7384e-09 - val_mse: 4.7384e-09\n",
      "Epoch 443/600\n",
      "949/949 [==============================] - 1s 810us/step - loss: 4.5583e-09 - mse: 4.5583e-09 - val_loss: 4.7483e-09 - val_mse: 4.7483e-09\n",
      "Epoch 444/600\n",
      "949/949 [==============================] - 1s 819us/step - loss: 4.5552e-09 - mse: 4.5552e-09 - val_loss: 4.7494e-09 - val_mse: 4.7494e-09\n",
      "Epoch 445/600\n",
      "949/949 [==============================] - 1s 823us/step - loss: 4.5573e-09 - mse: 4.5573e-09 - val_loss: 4.7379e-09 - val_mse: 4.7379e-09\n",
      "Epoch 446/600\n",
      "949/949 [==============================] - 1s 815us/step - loss: 4.5568e-09 - mse: 4.5568e-09 - val_loss: 4.7435e-09 - val_mse: 4.7435e-09\n",
      "Epoch 447/600\n",
      "949/949 [==============================] - 1s 815us/step - loss: 4.5583e-09 - mse: 4.5583e-09 - val_loss: 4.7432e-09 - val_mse: 4.7432e-09\n",
      "Epoch 448/600\n",
      "949/949 [==============================] - 1s 819us/step - loss: 4.5563e-09 - mse: 4.5563e-09 - val_loss: 4.7652e-09 - val_mse: 4.7652e-09\n",
      "Epoch 449/600\n",
      "949/949 [==============================] - 1s 821us/step - loss: 4.5573e-09 - mse: 4.5573e-09 - val_loss: 4.7533e-09 - val_mse: 4.7533e-09\n",
      "Epoch 450/600\n",
      "949/949 [==============================] - 1s 815us/step - loss: 4.5563e-09 - mse: 4.5563e-09 - val_loss: 4.7404e-09 - val_mse: 4.7404e-09\n",
      "Epoch 451/600\n",
      "949/949 [==============================] - 1s 819us/step - loss: 4.5560e-09 - mse: 4.5560e-09 - val_loss: 4.7479e-09 - val_mse: 4.7479e-09\n",
      "Epoch 452/600\n",
      "949/949 [==============================] - 1s 819us/step - loss: 4.5567e-09 - mse: 4.5567e-09 - val_loss: 4.7424e-09 - val_mse: 4.7424e-09\n",
      "Epoch 453/600\n",
      "949/949 [==============================] - 1s 815us/step - loss: 4.5555e-09 - mse: 4.5555e-09 - val_loss: 4.7482e-09 - val_mse: 4.7482e-09\n",
      "Epoch 454/600\n",
      "949/949 [==============================] - 1s 827us/step - loss: 4.5559e-09 - mse: 4.5559e-09 - val_loss: 4.7592e-09 - val_mse: 4.7592e-09\n",
      "Epoch 455/600\n",
      "949/949 [==============================] - 1s 831us/step - loss: 4.5543e-09 - mse: 4.5543e-09 - val_loss: 4.7361e-09 - val_mse: 4.7361e-09\n",
      "Epoch 456/600\n",
      "949/949 [==============================] - 1s 808us/step - loss: 4.5560e-09 - mse: 4.5560e-09 - val_loss: 4.7367e-09 - val_mse: 4.7367e-09\n",
      "Epoch 457/600\n",
      "949/949 [==============================] - 1s 810us/step - loss: 4.5563e-09 - mse: 4.5563e-09 - val_loss: 4.7569e-09 - val_mse: 4.7569e-09\n",
      "Epoch 458/600\n",
      "949/949 [==============================] - 1s 810us/step - loss: 4.5566e-09 - mse: 4.5566e-09 - val_loss: 4.7471e-09 - val_mse: 4.7471e-09\n",
      "Epoch 459/600\n",
      "949/949 [==============================] - 1s 813us/step - loss: 4.5570e-09 - mse: 4.5570e-09 - val_loss: 4.7402e-09 - val_mse: 4.7402e-09\n",
      "Epoch 460/600\n",
      "949/949 [==============================] - 1s 827us/step - loss: 4.5575e-09 - mse: 4.5575e-09 - val_loss: 4.7414e-09 - val_mse: 4.7414e-09\n",
      "Epoch 461/600\n",
      "949/949 [==============================] - 1s 819us/step - loss: 4.5558e-09 - mse: 4.5558e-09 - val_loss: 4.7496e-09 - val_mse: 4.7496e-09\n",
      "Epoch 462/600\n",
      "949/949 [==============================] - 1s 823us/step - loss: 4.5575e-09 - mse: 4.5575e-09 - val_loss: 4.7494e-09 - val_mse: 4.7494e-09\n",
      "Epoch 463/600\n",
      "949/949 [==============================] - 1s 831us/step - loss: 4.5524e-09 - mse: 4.5524e-09 - val_loss: 4.7447e-09 - val_mse: 4.7447e-09\n",
      "Epoch 464/600\n",
      "949/949 [==============================] - 1s 823us/step - loss: 4.5500e-09 - mse: 4.5500e-09 - val_loss: 4.7341e-09 - val_mse: 4.7341e-09\n",
      "Epoch 465/600\n",
      "949/949 [==============================] - 1s 836us/step - loss: 4.5509e-09 - mse: 4.5509e-09 - val_loss: 4.7355e-09 - val_mse: 4.7355e-09\n",
      "Epoch 466/600\n",
      "949/949 [==============================] - 1s 824us/step - loss: 4.5487e-09 - mse: 4.5487e-09 - val_loss: 4.7387e-09 - val_mse: 4.7387e-09\n",
      "Epoch 467/600\n",
      "949/949 [==============================] - 1s 819us/step - loss: 4.5493e-09 - mse: 4.5493e-09 - val_loss: 4.7391e-09 - val_mse: 4.7391e-09\n",
      "Epoch 468/600\n",
      "949/949 [==============================] - 1s 815us/step - loss: 4.5498e-09 - mse: 4.5498e-09 - val_loss: 4.7383e-09 - val_mse: 4.7383e-09\n",
      "Epoch 469/600\n",
      "949/949 [==============================] - 1s 819us/step - loss: 4.5502e-09 - mse: 4.5502e-09 - val_loss: 4.7360e-09 - val_mse: 4.7360e-09\n",
      "Epoch 470/600\n",
      "949/949 [==============================] - 1s 823us/step - loss: 4.5474e-09 - mse: 4.5474e-09 - val_loss: 4.7584e-09 - val_mse: 4.7584e-09\n",
      "Epoch 471/600\n",
      "949/949 [==============================] - 1s 823us/step - loss: 4.5510e-09 - mse: 4.5510e-09 - val_loss: 4.7356e-09 - val_mse: 4.7356e-09\n",
      "Epoch 472/600\n",
      "949/949 [==============================] - 1s 831us/step - loss: 4.5496e-09 - mse: 4.5496e-09 - val_loss: 4.7343e-09 - val_mse: 4.7343e-09\n",
      "Epoch 473/600\n",
      "949/949 [==============================] - 1s 836us/step - loss: 4.5490e-09 - mse: 4.5490e-09 - val_loss: 4.7449e-09 - val_mse: 4.7449e-09\n",
      "Epoch 474/600\n",
      "949/949 [==============================] - 1s 819us/step - loss: 4.5505e-09 - mse: 4.5505e-09 - val_loss: 4.7421e-09 - val_mse: 4.7421e-09\n",
      "Epoch 475/600\n",
      "949/949 [==============================] - 1s 823us/step - loss: 4.5494e-09 - mse: 4.5494e-09 - val_loss: 4.7397e-09 - val_mse: 4.7397e-09\n",
      "Epoch 476/600\n",
      "949/949 [==============================] - 1s 810us/step - loss: 4.5479e-09 - mse: 4.5479e-09 - val_loss: 4.7524e-09 - val_mse: 4.7524e-09\n",
      "Epoch 477/600\n",
      "949/949 [==============================] - 1s 813us/step - loss: 4.5490e-09 - mse: 4.5490e-09 - val_loss: 4.7354e-09 - val_mse: 4.7354e-09\n",
      "Epoch 478/600\n",
      "949/949 [==============================] - 1s 815us/step - loss: 4.5490e-09 - mse: 4.5490e-09 - val_loss: 4.7419e-09 - val_mse: 4.7419e-09\n",
      "Epoch 479/600\n",
      "949/949 [==============================] - 1s 815us/step - loss: 4.5496e-09 - mse: 4.5496e-09 - val_loss: 4.7410e-09 - val_mse: 4.7410e-09\n",
      "Epoch 480/600\n",
      "949/949 [==============================] - 1s 814us/step - loss: 4.5481e-09 - mse: 4.5481e-09 - val_loss: 4.7366e-09 - val_mse: 4.7366e-09\n",
      "Epoch 481/600\n",
      "949/949 [==============================] - 1s 815us/step - loss: 4.5491e-09 - mse: 4.5491e-09 - val_loss: 4.7386e-09 - val_mse: 4.7386e-09\n",
      "Epoch 482/600\n",
      "949/949 [==============================] - 1s 819us/step - loss: 4.5497e-09 - mse: 4.5497e-09 - val_loss: 4.7446e-09 - val_mse: 4.7446e-09\n",
      "Epoch 483/600\n",
      "949/949 [==============================] - 1s 815us/step - loss: 4.5485e-09 - mse: 4.5485e-09 - val_loss: 4.7412e-09 - val_mse: 4.7412e-09\n",
      "Epoch 484/600\n",
      "949/949 [==============================] - 1s 831us/step - loss: 4.5497e-09 - mse: 4.5497e-09 - val_loss: 4.7395e-09 - val_mse: 4.7395e-09\n",
      "Epoch 485/600\n",
      "949/949 [==============================] - 1s 819us/step - loss: 4.5487e-09 - mse: 4.5487e-09 - val_loss: 4.7425e-09 - val_mse: 4.7425e-09\n",
      "Epoch 486/600\n",
      "949/949 [==============================] - 1s 819us/step - loss: 4.5504e-09 - mse: 4.5504e-09 - val_loss: 4.7361e-09 - val_mse: 4.7361e-09\n",
      "Epoch 487/600\n",
      "949/949 [==============================] - 1s 823us/step - loss: 4.5485e-09 - mse: 4.5485e-09 - val_loss: 4.7407e-09 - val_mse: 4.7407e-09\n",
      "Epoch 488/600\n",
      "949/949 [==============================] - 1s 819us/step - loss: 4.5496e-09 - mse: 4.5496e-09 - val_loss: 4.7371e-09 - val_mse: 4.7371e-09\n",
      "Epoch 489/600\n",
      "949/949 [==============================] - 1s 823us/step - loss: 4.5456e-09 - mse: 4.5456e-09 - val_loss: 4.7359e-09 - val_mse: 4.7359e-09\n",
      "Epoch 490/600\n",
      "949/949 [==============================] - 1s 818us/step - loss: 4.5455e-09 - mse: 4.5455e-09 - val_loss: 4.7346e-09 - val_mse: 4.7346e-09\n",
      "Epoch 491/600\n",
      "949/949 [==============================] - 1s 827us/step - loss: 4.5452e-09 - mse: 4.5452e-09 - val_loss: 4.7358e-09 - val_mse: 4.7358e-09\n",
      "Epoch 492/600\n",
      "949/949 [==============================] - 1s 827us/step - loss: 4.5457e-09 - mse: 4.5457e-09 - val_loss: 4.7351e-09 - val_mse: 4.7351e-09\n",
      "Epoch 493/600\n",
      "949/949 [==============================] - 1s 823us/step - loss: 4.5453e-09 - mse: 4.5453e-09 - val_loss: 4.7335e-09 - val_mse: 4.7335e-09\n",
      "Epoch 494/600\n",
      "949/949 [==============================] - 1s 815us/step - loss: 4.5444e-09 - mse: 4.5444e-09 - val_loss: 4.7377e-09 - val_mse: 4.7377e-09\n",
      "Epoch 495/600\n",
      "949/949 [==============================] - 1s 818us/step - loss: 4.5451e-09 - mse: 4.5451e-09 - val_loss: 4.7328e-09 - val_mse: 4.7328e-09\n",
      "Epoch 496/600\n",
      "949/949 [==============================] - 1s 819us/step - loss: 4.5455e-09 - mse: 4.5455e-09 - val_loss: 4.7378e-09 - val_mse: 4.7378e-09\n",
      "Epoch 497/600\n",
      "949/949 [==============================] - 1s 814us/step - loss: 4.5460e-09 - mse: 4.5460e-09 - val_loss: 4.7320e-09 - val_mse: 4.7320e-09\n",
      "Epoch 498/600\n",
      "949/949 [==============================] - 1s 823us/step - loss: 4.5444e-09 - mse: 4.5444e-09 - val_loss: 4.7385e-09 - val_mse: 4.7385e-09\n",
      "Epoch 499/600\n",
      "949/949 [==============================] - 1s 827us/step - loss: 4.5448e-09 - mse: 4.5448e-09 - val_loss: 4.7391e-09 - val_mse: 4.7391e-09\n",
      "Epoch 500/600\n",
      "949/949 [==============================] - 1s 817us/step - loss: 4.5455e-09 - mse: 4.5455e-09 - val_loss: 4.7326e-09 - val_mse: 4.7326e-09\n",
      "Epoch 501/600\n",
      "949/949 [==============================] - 1s 810us/step - loss: 4.5448e-09 - mse: 4.5448e-09 - val_loss: 4.7337e-09 - val_mse: 4.7337e-09\n",
      "Epoch 502/600\n",
      "949/949 [==============================] - 1s 823us/step - loss: 4.5455e-09 - mse: 4.5455e-09 - val_loss: 4.7383e-09 - val_mse: 4.7383e-09\n",
      "Epoch 503/600\n",
      "949/949 [==============================] - 1s 819us/step - loss: 4.5455e-09 - mse: 4.5455e-09 - val_loss: 4.7357e-09 - val_mse: 4.7357e-09\n",
      "Epoch 504/600\n",
      "949/949 [==============================] - 1s 836us/step - loss: 4.5453e-09 - mse: 4.5453e-09 - val_loss: 4.7328e-09 - val_mse: 4.7328e-09\n",
      "Epoch 505/600\n",
      "949/949 [==============================] - 1s 819us/step - loss: 4.5452e-09 - mse: 4.5452e-09 - val_loss: 4.7345e-09 - val_mse: 4.7345e-09\n",
      "Epoch 506/600\n",
      "949/949 [==============================] - 1s 819us/step - loss: 4.5452e-09 - mse: 4.5452e-09 - val_loss: 4.7338e-09 - val_mse: 4.7338e-09\n",
      "Epoch 507/600\n",
      "949/949 [==============================] - 1s 810us/step - loss: 4.5445e-09 - mse: 4.5445e-09 - val_loss: 4.7407e-09 - val_mse: 4.7407e-09\n",
      "Epoch 508/600\n",
      "949/949 [==============================] - 1s 823us/step - loss: 4.5459e-09 - mse: 4.5459e-09 - val_loss: 4.7356e-09 - val_mse: 4.7356e-09\n",
      "Epoch 509/600\n",
      "949/949 [==============================] - 1s 815us/step - loss: 4.5457e-09 - mse: 4.5457e-09 - val_loss: 4.7335e-09 - val_mse: 4.7335e-09\n",
      "Epoch 510/600\n",
      "949/949 [==============================] - 1s 814us/step - loss: 4.5442e-09 - mse: 4.5442e-09 - val_loss: 4.7351e-09 - val_mse: 4.7351e-09\n",
      "Epoch 511/600\n",
      "949/949 [==============================] - 1s 831us/step - loss: 4.5438e-09 - mse: 4.5438e-09 - val_loss: 4.7381e-09 - val_mse: 4.7381e-09\n",
      "Epoch 512/600\n",
      "949/949 [==============================] - 1s 819us/step - loss: 4.5452e-09 - mse: 4.5452e-09 - val_loss: 4.7354e-09 - val_mse: 4.7354e-09\n",
      "Epoch 513/600\n",
      "949/949 [==============================] - 1s 823us/step - loss: 4.5447e-09 - mse: 4.5447e-09 - val_loss: 4.7333e-09 - val_mse: 4.7333e-09\n",
      "Epoch 514/600\n",
      "949/949 [==============================] - 1s 819us/step - loss: 4.5449e-09 - mse: 4.5449e-09 - val_loss: 4.7335e-09 - val_mse: 4.7335e-09\n",
      "Epoch 515/600\n",
      "949/949 [==============================] - 1s 823us/step - loss: 4.5427e-09 - mse: 4.5427e-09 - val_loss: 4.7356e-09 - val_mse: 4.7356e-09\n",
      "Epoch 516/600\n",
      "949/949 [==============================] - 1s 827us/step - loss: 4.5430e-09 - mse: 4.5430e-09 - val_loss: 4.7384e-09 - val_mse: 4.7384e-09\n",
      "Epoch 517/600\n",
      "949/949 [==============================] - 1s 823us/step - loss: 4.5431e-09 - mse: 4.5431e-09 - val_loss: 4.7334e-09 - val_mse: 4.7334e-09\n",
      "Epoch 518/600\n",
      "949/949 [==============================] - 1s 823us/step - loss: 4.5430e-09 - mse: 4.5430e-09 - val_loss: 4.7374e-09 - val_mse: 4.7374e-09\n",
      "Epoch 519/600\n",
      "949/949 [==============================] - 1s 827us/step - loss: 4.5432e-09 - mse: 4.5432e-09 - val_loss: 4.7316e-09 - val_mse: 4.7316e-09\n",
      "Epoch 520/600\n",
      "949/949 [==============================] - 1s 819us/step - loss: 4.5427e-09 - mse: 4.5427e-09 - val_loss: 4.7343e-09 - val_mse: 4.7343e-09\n",
      "Epoch 521/600\n",
      "949/949 [==============================] - 1s 823us/step - loss: 4.5430e-09 - mse: 4.5430e-09 - val_loss: 4.7325e-09 - val_mse: 4.7325e-09\n",
      "Epoch 522/600\n",
      "949/949 [==============================] - 1s 815us/step - loss: 4.5426e-09 - mse: 4.5426e-09 - val_loss: 4.7329e-09 - val_mse: 4.7329e-09\n",
      "Epoch 523/600\n",
      "949/949 [==============================] - 1s 818us/step - loss: 4.5427e-09 - mse: 4.5427e-09 - val_loss: 4.7334e-09 - val_mse: 4.7334e-09\n",
      "Epoch 524/600\n",
      "949/949 [==============================] - 1s 827us/step - loss: 4.5425e-09 - mse: 4.5425e-09 - val_loss: 4.7329e-09 - val_mse: 4.7329e-09\n",
      "Epoch 525/600\n",
      "949/949 [==============================] - 1s 823us/step - loss: 4.5431e-09 - mse: 4.5431e-09 - val_loss: 4.7327e-09 - val_mse: 4.7327e-09\n",
      "Epoch 526/600\n",
      "949/949 [==============================] - 1s 821us/step - loss: 4.5426e-09 - mse: 4.5426e-09 - val_loss: 4.7336e-09 - val_mse: 4.7336e-09\n",
      "Epoch 527/600\n",
      "949/949 [==============================] - 1s 823us/step - loss: 4.5421e-09 - mse: 4.5421e-09 - val_loss: 4.7317e-09 - val_mse: 4.7317e-09\n",
      "Epoch 528/600\n",
      "949/949 [==============================] - 1s 815us/step - loss: 4.5432e-09 - mse: 4.5432e-09 - val_loss: 4.7330e-09 - val_mse: 4.7330e-09\n",
      "Epoch 529/600\n",
      "949/949 [==============================] - 1s 814us/step - loss: 4.5431e-09 - mse: 4.5431e-09 - val_loss: 4.7325e-09 - val_mse: 4.7325e-09\n",
      "Epoch 530/600\n",
      "949/949 [==============================] - 1s 815us/step - loss: 4.5428e-09 - mse: 4.5428e-09 - val_loss: 4.7347e-09 - val_mse: 4.7347e-09\n",
      "Epoch 531/600\n",
      "949/949 [==============================] - 1s 810us/step - loss: 4.5422e-09 - mse: 4.5422e-09 - val_loss: 4.7361e-09 - val_mse: 4.7361e-09\n",
      "Epoch 532/600\n",
      "949/949 [==============================] - 1s 819us/step - loss: 4.5430e-09 - mse: 4.5430e-09 - val_loss: 4.7326e-09 - val_mse: 4.7326e-09\n",
      "Epoch 533/600\n",
      "949/949 [==============================] - 1s 819us/step - loss: 4.5430e-09 - mse: 4.5430e-09 - val_loss: 4.7312e-09 - val_mse: 4.7312e-09\n",
      "Epoch 534/600\n",
      "949/949 [==============================] - 1s 815us/step - loss: 4.5428e-09 - mse: 4.5428e-09 - val_loss: 4.7330e-09 - val_mse: 4.7330e-09\n",
      "Epoch 535/600\n",
      "949/949 [==============================] - 1s 819us/step - loss: 4.5434e-09 - mse: 4.5434e-09 - val_loss: 4.7319e-09 - val_mse: 4.7319e-09\n",
      "Epoch 536/600\n",
      "949/949 [==============================] - 1s 827us/step - loss: 4.5426e-09 - mse: 4.5426e-09 - val_loss: 4.7334e-09 - val_mse: 4.7334e-09\n",
      "Epoch 537/600\n",
      "949/949 [==============================] - 1s 823us/step - loss: 4.5428e-09 - mse: 4.5428e-09 - val_loss: 4.7324e-09 - val_mse: 4.7324e-09\n",
      "Epoch 538/600\n",
      "949/949 [==============================] - 1s 811us/step - loss: 4.5427e-09 - mse: 4.5427e-09 - val_loss: 4.7349e-09 - val_mse: 4.7349e-09\n",
      "Epoch 539/600\n",
      "949/949 [==============================] - 1s 810us/step - loss: 4.5428e-09 - mse: 4.5428e-09 - val_loss: 4.7328e-09 - val_mse: 4.7328e-09\n",
      "Epoch 540/600\n",
      "949/949 [==============================] - 1s 815us/step - loss: 4.5419e-09 - mse: 4.5419e-09 - val_loss: 4.7321e-09 - val_mse: 4.7321e-09\n",
      "Epoch 541/600\n",
      "949/949 [==============================] - 1s 823us/step - loss: 4.5413e-09 - mse: 4.5413e-09 - val_loss: 4.7329e-09 - val_mse: 4.7329e-09\n",
      "Epoch 542/600\n",
      "949/949 [==============================] - 1s 831us/step - loss: 4.5415e-09 - mse: 4.5415e-09 - val_loss: 4.7307e-09 - val_mse: 4.7307e-09\n",
      "Epoch 543/600\n",
      "949/949 [==============================] - 1s 844us/step - loss: 4.5417e-09 - mse: 4.5417e-09 - val_loss: 4.7312e-09 - val_mse: 4.7312e-09\n",
      "Epoch 544/600\n",
      "949/949 [==============================] - 1s 824us/step - loss: 4.5410e-09 - mse: 4.5410e-09 - val_loss: 4.7322e-09 - val_mse: 4.7322e-09\n",
      "Epoch 545/600\n",
      "949/949 [==============================] - 1s 827us/step - loss: 4.5413e-09 - mse: 4.5413e-09 - val_loss: 4.7315e-09 - val_mse: 4.7315e-09\n",
      "Epoch 546/600\n",
      "949/949 [==============================] - 1s 832us/step - loss: 4.5413e-09 - mse: 4.5413e-09 - val_loss: 4.7315e-09 - val_mse: 4.7315e-09\n",
      "Epoch 547/600\n",
      "949/949 [==============================] - 1s 827us/step - loss: 4.5416e-09 - mse: 4.5416e-09 - val_loss: 4.7313e-09 - val_mse: 4.7313e-09\n",
      "Epoch 548/600\n",
      "949/949 [==============================] - 1s 823us/step - loss: 4.5416e-09 - mse: 4.5416e-09 - val_loss: 4.7319e-09 - val_mse: 4.7319e-09\n",
      "Epoch 549/600\n",
      "949/949 [==============================] - 1s 823us/step - loss: 4.5411e-09 - mse: 4.5411e-09 - val_loss: 4.7331e-09 - val_mse: 4.7331e-09\n",
      "Epoch 550/600\n",
      "949/949 [==============================] - 1s 823us/step - loss: 4.5416e-09 - mse: 4.5416e-09 - val_loss: 4.7333e-09 - val_mse: 4.7333e-09\n",
      "Epoch 551/600\n",
      "949/949 [==============================] - 1s 836us/step - loss: 4.5418e-09 - mse: 4.5418e-09 - val_loss: 4.7330e-09 - val_mse: 4.7330e-09\n",
      "Epoch 552/600\n",
      "949/949 [==============================] - 1s 827us/step - loss: 4.5416e-09 - mse: 4.5416e-09 - val_loss: 4.7319e-09 - val_mse: 4.7319e-09\n",
      "Epoch 553/600\n",
      "949/949 [==============================] - 1s 836us/step - loss: 4.5415e-09 - mse: 4.5415e-09 - val_loss: 4.7319e-09 - val_mse: 4.7319e-09\n",
      "Epoch 554/600\n",
      "949/949 [==============================] - 1s 831us/step - loss: 4.5417e-09 - mse: 4.5417e-09 - val_loss: 4.7331e-09 - val_mse: 4.7331e-09\n",
      "Epoch 555/600\n",
      "949/949 [==============================] - 1s 827us/step - loss: 4.5412e-09 - mse: 4.5412e-09 - val_loss: 4.7326e-09 - val_mse: 4.7326e-09\n",
      "Epoch 556/600\n",
      "949/949 [==============================] - 1s 827us/step - loss: 4.5412e-09 - mse: 4.5412e-09 - val_loss: 4.7314e-09 - val_mse: 4.7314e-09\n",
      "Epoch 557/600\n",
      "949/949 [==============================] - 1s 832us/step - loss: 4.5410e-09 - mse: 4.5410e-09 - val_loss: 4.7333e-09 - val_mse: 4.7333e-09\n",
      "Epoch 558/600\n",
      "949/949 [==============================] - 1s 827us/step - loss: 4.5418e-09 - mse: 4.5418e-09 - val_loss: 4.7318e-09 - val_mse: 4.7318e-09\n",
      "Epoch 559/600\n",
      "949/949 [==============================] - 1s 832us/step - loss: 4.5412e-09 - mse: 4.5412e-09 - val_loss: 4.7311e-09 - val_mse: 4.7311e-09\n",
      "Epoch 560/600\n",
      "949/949 [==============================] - 1s 823us/step - loss: 4.5414e-09 - mse: 4.5414e-09 - val_loss: 4.7327e-09 - val_mse: 4.7327e-09\n",
      "Epoch 561/600\n",
      "949/949 [==============================] - 1s 823us/step - loss: 4.5412e-09 - mse: 4.5412e-09 - val_loss: 4.7317e-09 - val_mse: 4.7317e-09\n",
      "Epoch 562/600\n",
      "949/949 [==============================] - 1s 827us/step - loss: 4.5418e-09 - mse: 4.5418e-09 - val_loss: 4.7325e-09 - val_mse: 4.7325e-09\n",
      "Epoch 563/600\n",
      "949/949 [==============================] - 1s 815us/step - loss: 4.5414e-09 - mse: 4.5414e-09 - val_loss: 4.7313e-09 - val_mse: 4.7313e-09\n",
      "Epoch 564/600\n",
      "949/949 [==============================] - 1s 827us/step - loss: 4.5417e-09 - mse: 4.5417e-09 - val_loss: 4.7314e-09 - val_mse: 4.7314e-09\n",
      "Epoch 565/600\n",
      "949/949 [==============================] - 1s 822us/step - loss: 4.5414e-09 - mse: 4.5414e-09 - val_loss: 4.7316e-09 - val_mse: 4.7316e-09\n",
      "Epoch 566/600\n",
      "949/949 [==============================] - 1s 827us/step - loss: 4.5410e-09 - mse: 4.5410e-09 - val_loss: 4.7312e-09 - val_mse: 4.7312e-09\n",
      "Epoch 567/600\n",
      "949/949 [==============================] - 1s 827us/step - loss: 4.5410e-09 - mse: 4.5410e-09 - val_loss: 4.7314e-09 - val_mse: 4.7314e-09\n",
      "Epoch 568/600\n",
      "949/949 [==============================] - 1s 815us/step - loss: 4.5405e-09 - mse: 4.5405e-09 - val_loss: 4.7311e-09 - val_mse: 4.7311e-09\n",
      "Epoch 569/600\n",
      "949/949 [==============================] - 1s 827us/step - loss: 4.5408e-09 - mse: 4.5408e-09 - val_loss: 4.7313e-09 - val_mse: 4.7313e-09\n",
      "Epoch 570/600\n",
      "949/949 [==============================] - 1s 832us/step - loss: 4.5406e-09 - mse: 4.5406e-09 - val_loss: 4.7314e-09 - val_mse: 4.7314e-09\n",
      "Epoch 571/600\n",
      "949/949 [==============================] - 1s 832us/step - loss: 4.5407e-09 - mse: 4.5407e-09 - val_loss: 4.7314e-09 - val_mse: 4.7314e-09\n",
      "Epoch 572/600\n",
      "949/949 [==============================] - 1s 840us/step - loss: 4.5408e-09 - mse: 4.5408e-09 - val_loss: 4.7312e-09 - val_mse: 4.7312e-09\n",
      "Epoch 573/600\n",
      "949/949 [==============================] - 1s 827us/step - loss: 4.5408e-09 - mse: 4.5408e-09 - val_loss: 4.7324e-09 - val_mse: 4.7324e-09\n",
      "Epoch 574/600\n",
      "949/949 [==============================] - 1s 819us/step - loss: 4.5406e-09 - mse: 4.5406e-09 - val_loss: 4.7320e-09 - val_mse: 4.7320e-09\n",
      "Epoch 575/600\n",
      "949/949 [==============================] - 1s 819us/step - loss: 4.5406e-09 - mse: 4.5406e-09 - val_loss: 4.7317e-09 - val_mse: 4.7317e-09\n",
      "Epoch 576/600\n",
      "949/949 [==============================] - 1s 812us/step - loss: 4.5406e-09 - mse: 4.5406e-09 - val_loss: 4.7315e-09 - val_mse: 4.7315e-09\n",
      "Epoch 577/600\n",
      "949/949 [==============================] - 1s 810us/step - loss: 4.5406e-09 - mse: 4.5406e-09 - val_loss: 4.7315e-09 - val_mse: 4.7315e-09\n",
      "Epoch 578/600\n",
      "949/949 [==============================] - 1s 819us/step - loss: 4.5409e-09 - mse: 4.5409e-09 - val_loss: 4.7317e-09 - val_mse: 4.7317e-09\n",
      "Epoch 579/600\n",
      "949/949 [==============================] - 1s 815us/step - loss: 4.5406e-09 - mse: 4.5406e-09 - val_loss: 4.7312e-09 - val_mse: 4.7312e-09\n",
      "Epoch 580/600\n",
      "949/949 [==============================] - 1s 810us/step - loss: 4.5404e-09 - mse: 4.5404e-09 - val_loss: 4.7329e-09 - val_mse: 4.7329e-09\n",
      "Epoch 581/600\n",
      "949/949 [==============================] - 1s 815us/step - loss: 4.5409e-09 - mse: 4.5409e-09 - val_loss: 4.7313e-09 - val_mse: 4.7313e-09\n",
      "Epoch 582/600\n",
      "949/949 [==============================] - 1s 823us/step - loss: 4.5409e-09 - mse: 4.5409e-09 - val_loss: 4.7319e-09 - val_mse: 4.7319e-09\n",
      "Epoch 583/600\n",
      "949/949 [==============================] - 1s 823us/step - loss: 4.5407e-09 - mse: 4.5407e-09 - val_loss: 4.7322e-09 - val_mse: 4.7322e-09\n",
      "Epoch 584/600\n",
      "949/949 [==============================] - 1s 832us/step - loss: 4.5405e-09 - mse: 4.5405e-09 - val_loss: 4.7319e-09 - val_mse: 4.7319e-09\n",
      "Epoch 585/600\n",
      "949/949 [==============================] - 1s 815us/step - loss: 4.5408e-09 - mse: 4.5408e-09 - val_loss: 4.7321e-09 - val_mse: 4.7321e-09\n",
      "Epoch 586/600\n",
      "949/949 [==============================] - 1s 815us/step - loss: 4.5405e-09 - mse: 4.5405e-09 - val_loss: 4.7312e-09 - val_mse: 4.7312e-09\n",
      "Epoch 587/600\n",
      "949/949 [==============================] - 1s 819us/step - loss: 4.5409e-09 - mse: 4.5409e-09 - val_loss: 4.7309e-09 - val_mse: 4.7309e-09\n",
      "Epoch 588/600\n",
      "949/949 [==============================] - 1s 823us/step - loss: 4.5406e-09 - mse: 4.5406e-09 - val_loss: 4.7315e-09 - val_mse: 4.7315e-09\n",
      "Epoch 589/600\n",
      "949/949 [==============================] - 1s 823us/step - loss: 4.5402e-09 - mse: 4.5402e-09 - val_loss: 4.7314e-09 - val_mse: 4.7314e-09\n",
      "Epoch 590/600\n",
      "949/949 [==============================] - 1s 810us/step - loss: 4.5409e-09 - mse: 4.5409e-09 - val_loss: 4.7320e-09 - val_mse: 4.7320e-09\n",
      "Epoch 591/600\n",
      "949/949 [==============================] - 1s 827us/step - loss: 4.5405e-09 - mse: 4.5405e-09 - val_loss: 4.7310e-09 - val_mse: 4.7310e-09\n",
      "Epoch 592/600\n",
      "949/949 [==============================] - 1s 823us/step - loss: 4.5404e-09 - mse: 4.5404e-09 - val_loss: 4.7308e-09 - val_mse: 4.7308e-09\n",
      "Epoch 593/600\n",
      "949/949 [==============================] - 1s 819us/step - loss: 4.5402e-09 - mse: 4.5402e-09 - val_loss: 4.7311e-09 - val_mse: 4.7311e-09\n",
      "Epoch 594/600\n",
      "949/949 [==============================] - 1s 810us/step - loss: 4.5402e-09 - mse: 4.5402e-09 - val_loss: 4.7311e-09 - val_mse: 4.7311e-09\n",
      "Epoch 595/600\n",
      "949/949 [==============================] - 1s 810us/step - loss: 4.5403e-09 - mse: 4.5403e-09 - val_loss: 4.7312e-09 - val_mse: 4.7312e-09\n",
      "Epoch 596/600\n",
      "949/949 [==============================] - 1s 808us/step - loss: 4.5403e-09 - mse: 4.5403e-09 - val_loss: 4.7309e-09 - val_mse: 4.7309e-09\n",
      "Epoch 597/600\n",
      "949/949 [==============================] - 1s 806us/step - loss: 4.5402e-09 - mse: 4.5402e-09 - val_loss: 4.7315e-09 - val_mse: 4.7315e-09\n",
      "Epoch 598/600\n",
      "949/949 [==============================] - 1s 806us/step - loss: 4.5402e-09 - mse: 4.5402e-09 - val_loss: 4.7311e-09 - val_mse: 4.7311e-09\n",
      "Epoch 599/600\n",
      "949/949 [==============================] - 1s 823us/step - loss: 4.5402e-09 - mse: 4.5402e-09 - val_loss: 4.7313e-09 - val_mse: 4.7313e-09\n",
      "Epoch 600/600\n",
      "949/949 [==============================] - 1s 815us/step - loss: 4.5401e-09 - mse: 4.5401e-09 - val_loss: 4.7314e-09 - val_mse: 4.7314e-09\n",
      "Elapsed time:  466.8300226\n",
      "[4.731393055124045e-09, 4.731393055124045e-09]\n"
     ]
    }
   ],
   "source": [
    "# build the model and train it\n",
    "initial_learning_rate = 0.020669824682365133\n",
    "lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n",
    "  initial_learning_rate,\n",
    "  decay_steps=375*65,\n",
    "  decay_rate=0.575401007701697,\n",
    "  staircase=True)\n",
    "\n",
    "Normlayer1=tf.keras.layers.Normalization()\n",
    "Normlayer1.adapt(inputs_real)\n",
    "\n",
    "model = tf.keras.Sequential()\n",
    "model.add(Normlayer1)\n",
    "model.add(tf.keras.layers.Dense(43, activation='sigmoid'))\n",
    "model.add(tf.keras.layers.Dense(56, activation='sigmoid'))\n",
    "model.add(tf.keras.layers.Dense(43, activation='sigmoid'))\n",
    "model.add(tf.keras.layers.Dense(8))\n",
    "\n",
    "#Train using Adam\n",
    "opt = tf.keras.optimizers.Adam(learning_rate=lr_schedule)\n",
    "model.compile(optimizer=opt, loss='mse', metrics=['mse'])\n",
    "\n",
    "t2_start = time.perf_counter()\n",
    "history = model.fit(inputs_train, outputs_train, epochs=600, batch_size=16, validation_data=(inputs_test, outputs_test))\n",
    "t2_stop = time.perf_counter()\n",
    "print(\"Elapsed time: \", t2_stop - t2_start)\n",
    "\n",
    "print(model.evaluate(inputs_test, outputs_test, verbose=0))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-19T08:45:35.390306600Z",
     "start_time": "2023-06-19T08:37:47.917609500Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "data": {
      "text/plain": "<Figure size 648x432 with 1 Axes>",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[9.0, 6.0]\n"
     ]
    }
   ],
   "source": [
    "# # increase the size of the graphs. The default size is (6,4).\n",
    "plt.rcParams[\"figure.figsize\"] = (9,6)\n",
    "\n",
    "# summarize history for loss\n",
    "plt.plot(history.history['loss'], '-r.', label='Training Loss')\n",
    "plt.plot(history.history['val_loss'], '--b.', label='Validation Loss')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.semilogy()\n",
    "plt.grid()\n",
    "plt.show()\n",
    "\n",
    "print(plt.rcParams[\"figure.figsize\"])\n",
    "#print(history.history)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-19T08:45:35.796225900Z",
     "start_time": "2023-06-19T08:45:35.390306600Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-19T08:45:36.102916400Z",
     "start_time": "2023-06-19T08:45:35.796225900Z"
    }
   },
   "outputs": [],
   "source": [
    "Y_input=inputs_origin\n",
    "Y_pred=model(Y_input, training=False).numpy()\n",
    "Y_output=outputs_origin\n",
    "# Save inputs index\n",
    "with open('Yo20op1084_ana_inputs.csv','w', newline=\"\") as output_file:\n",
    "    writer = csv.writer(output_file)\n",
    "    writer.writerows(Y_input)\n",
    "# Save predcition from FNN\n",
    "with open('Yo20op1084_ana_prediction.csv','w', newline=\"\") as output_file:\n",
    "    writer = csv.writer(output_file)\n",
    "    writer.writerows(Y_pred)\n",
    "# Save original data\n",
    "with open('Yo20op1084_ana_outputs.csv','w', newline=\"\") as output_file:\n",
    "    writer = csv.writer(output_file)\n",
    "    writer.writerows(Y_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 473,
     "status": "ok",
     "timestamp": 1648264764413,
     "user": {
      "displayName": "Ramsfield Lee",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiHPffa6hxu7mszm5ZYTfN7ZGsy0zsXFpnUK65SWA=s64",
      "userId": "05718072999768408940"
     },
     "user_tz": 240
    },
    "id": "DblOFeB3pDsb",
    "outputId": "28f2f4b8-0442-47d7-daa6-4d0132d1e4a1",
    "scrolled": true,
    "tags": [],
    "ExecuteTime": {
     "end_time": "2023-06-19T08:45:36.347003100Z",
     "start_time": "2023-06-19T08:45:36.102916400Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "204/204 [==============================] - 0s 452us/step\n",
      "[[ 9.38175537e-05  1.12081022e-04  8.03860625e-06 ... -1.92422812e-05\n",
      "  -1.77292590e-05 -9.04752573e-05]\n",
      " [ 5.14047121e-05  2.35837489e-05  1.44368620e-04 ... -2.61954392e-05\n",
      "   1.95952973e-04  1.09314732e-04]\n",
      " [ 6.42121560e-05  7.53580358e-05 -1.45551848e-04 ...  1.85187215e-05\n",
      "   3.18161637e-05  1.75933057e-05]\n",
      " ...\n",
      " [-9.66687645e-05 -3.71552403e-05 -1.32291791e-04 ...  1.11554704e-05\n",
      "   4.12948014e-05 -5.98258030e-05]\n",
      " [ 1.16319311e-04 -1.35035474e-05  6.74277391e-05 ... -2.03845961e-05\n",
      "   1.11497607e-04  7.80707602e-05]\n",
      " [-1.34322071e-05 -1.02901134e-05  7.06403472e-05 ... -1.49404390e-05\n",
      "  -2.90006565e-05 -3.06419541e-05]]\n",
      "4.731392599000191e-09\n"
     ]
    }
   ],
   "source": [
    "# Use the model to predict test data\n",
    "output_pred = model.predict(inputs_test)\n",
    "#print(output_pred)\n",
    "output_diff = output_pred - outputs_test\n",
    "MSE = np.square(np.subtract(output_pred,outputs_test)).mean()\n",
    "print(output_diff)\n",
    "print(MSE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-19T08:45:36.358994Z",
     "start_time": "2023-06-19T08:45:36.347003100Z"
    }
   }
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "YdqFNN.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
